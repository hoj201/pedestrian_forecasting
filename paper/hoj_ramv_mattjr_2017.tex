%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{todonotes}

\usepackage{tikz}
\usetikzlibrary{arrows,positioning}

\newtheorem{thm}{Theorem}

\title{\LARGE \bf
Real-time Forecasting of Pedestrian Probability Densities*
}


\author{Henry O. Jacobs, Owen Hughes, Matt Johnson-Roberson, and Ram Vasudevan$^{2}$% <-this % stops a space
\thanks{*This work was supported by ???}% <-this % stops a space
\thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This electronic document is a ÒliveÓ template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

blah blah blah

\section{PREVIOUS WORK}

\subsection{Social Force Models}
Cite these papers \cite{Henderson1971,Helbing1992}

\subsection{MDP Models}
Cite these papers \cite{Kitani2012,Ballan2016,Karasev2016,Ziebart2008}

\section{DATA EXPLORATION}
Simply viewing some of the plots we can make some qualitative observations regarding trajectories.
We can use these observations to simplify our analysis of the data and build our prediction algorithm.

\subsection{Speed is piecewise constant}
	The speed of the agents appears to be constant over the course of a few seconds.
	Changes is speed occur abruptly, shifting from one constant to another.
	A typical plot of speed is displayed in figure \ref{fig:speed}.

%\begin{figure}[b] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=0.5\textwidth]{./figures/speed_plot.pdf} 
%   \caption{We observe that speed appears to be a sum of step functions with widths lasting a few seconds}
%   \label{fig:speed}
%\end{figure}

\subsection{There is substantial spatial structure}
We see that the vast majority of trajectories appear to fall under a small number of clusters.
A plot of all trajectories from one video is depicted in figure \ref{fig:trajectories}.
Later, we will exploit this structure by using affinity propagation to cluster the trajectories and produce a
finite set of classes of motion for each scene.

%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=3in]{./figures/trajectories.pdf} 
%   \caption{A plot of trajectories from one scene in the Stanford drone data-set}
%   \label{fig:trajectories}
%\end{figure}

\section{MODEL DESCRIPTION}
We assume each pedestrian is of two types, linear and nonlinear.
Given the initial position and velocity, $x_0$ and $v_0$,
the linear agents evolve according to the dynamics $x(t) = x_0 + t v_0$.
If the agent is nonlinear there are $K$ possible sub-types.
In particular, we assume that the dynamics for a nonlinear agent are prescribed by an ODE of the form $\dot{x} = v = s X_k(x)$
for some $k \in \{ 1,\dots, K\}$ and some $s \in \mathbb{R}$.
In all, the space of possible agents is parametrized by the set
\begin{align*}
	Typeset = \{ \emptyset \} \cup \{ (k,s) \mid k \in \{1,\dots,K\}, s \in \mathbb{R} \}
\end{align*}
If the type of an agent is $\emptyset$, then the agent satisfies linear dynamics.
Otherwise, the type of an agent is nonlinear, and of type $(k,s)$ for some $k \in \{1,\dots,K\}$ and some $s \in \mathbb{R}$.

If we denote the measurements of position $x$ by $\hat{x}$ and the measurement of velocity $v$ by $\hat{v}$
then the pgm for our model is given by

\begin{center}
\begin{tikzpicture}[thick, module/.style={rectangle,draw,thin,rounded corners,shade,top color=blue!50,minimum size = 4mm}]
	\node[module] (t) {Type};
	\node[module] (x) [right=of t] {$x$};
	\node[module] (x_hat) [right=of x] {$\hat{x}$};
	\draw[->] (x.east) to (x_hat.west);
	\node[module] (v) [below=of x] {$v$};
	\node[module] (v_hat) [right=of v] {$\hat{v}$};
	\draw[->] (v.east) to (v_hat.west);
	\draw[->] (t.east) to (x.west);
	\draw[->] (t.east) to (v.north west);
	\draw[->] (x.south) to (v.north);
\end{tikzpicture}
\end{center}

To initialize the model we will infer the probabilities $\Pr(t)$ for $t \in Typeset$.
as well as the vector-field $X_1,\dots,X_K$ from our data.
This is discussed in the next section.

From our model, it is clear that $\Pr( v \mid (k, s), x ) = \delta( v - s X_k (x) )$, where $\delta( \cdot )$ is the Dirac-delta distribution.
The other posteriors in the above pgm are regarded as modeling choices, and described in the appendix \todo{You still need to write this appendix}.


\section{Offline computations}
Offline, the primary task is to learn the classes for the agents.
This entails clustering observed trajectories, and constructing a vector-field for each cluster.

\subsubsection{Clustering}
For a fixed scene with a database of agent trajectories we cluster the trajectories by applying the Affinity propagation algorithm to the end-points.
We then prune the clusters by discarding trajectories which are outliers with respect to total length (we define an outlier using the standard inter-quartile range criterion with a IQR coefficient of $1.5$).
We then throw out clusters which contain less than $10\%$ of the trajectories.
We associate class labels, $c_1,\dots,c_n$,  to the remaining clusters, and we will develop a model for each of these classes in the next section.
We also add an additional class, $c_0$, where the underlying model will be a linear predictor.
Finally, we define a prior, $P(c)$ to compute the probability that a given agent falls within one of these classes.
We set $P(c_0) = ( \text{\# discarded trajectories} / \text{ \# trajectories } )$ and $P(c_k ) = ( \text{\# trajectories in cluster $c_k$} / \text{ \# trajectories } )$ for $k=1,\dots,n$.

\subsubsection{Compute the posterior of class given a position}
Given an agent located at $x \in D$, we would like to know the conditional probability
that this agent is of class $c$. That is, we'd like to compute $P(c \mid x)$.
Moreover, during runtime we will need this computation to be fast.
This rules out standard probabilistic classification schemes such as support vector-machines and other local voting based methods.
The CPU time to evaluate grows with the amount of data in these methods.

Instead we uses a probability density inspired by statistical mechanics.
First, we use Baye's theorem to convert the computation into choosing Likelihood $P( x \mid c)$.
For the linear-predictor class, $c_0$ we assume $P(x \mid c_0 )$ is a uniform distribution over our domain.

For classes $c_1, \dots, c_n$ we use the following strategy.
Given points $x_{1,k},\dots,x_{N,k}$ associated to class $c_k$ we define
\begin{align}
	P(x \mid c_k) = \frac{1}{Z_k } e^{-V_k(x) } \label{eq:x given c}
\end{align}
where $V_k(x)$ is the minimizer of
$$
	C[ V_k ] =  \log \left( \int e^{ - V_k(x) } dx \right) + \sum_{i=1}^N V_k(x_{i,k} ).
$$
over some finite-dimensional function space (perhaps a space of low order polynomials).
Physically, $V_k(x)$ is the most likely potential function given observations $\{ x_{i,k}\}_{k=1}^N$ under the assumption that points are drawn from \eqref{eq:x given c}.

Again, the advantage to this approach is that we may restrict $V_c$ to a class of quickly computable functions which will not slow down performance at runtime.
In this paper we restrict $V_c$ to be a sum of tensor products of low order Legendre polynomials.

%\begin{figure}[t] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=2in]{./figures/learned_potentials.pdf} 
%   \caption{Some learned potentials for computing $P(x|c_k)$}
%   \label{fig:learned potentials}
%\end{figure}

\subsubsection{Learning vector fields}
Given trajectories, we may fit a director field of the form $X(x,y) = ( \cos( \theta_\alpha(x,y) ) , \sin( \theta_\alpha(x,y) ) )$
where 
$$
	\theta_\alpha(x,y) = \sum_{ij} \alpha_{ij} L_i(x / w) L_j(y / h).
$$


Given observations $\vec{x}_0,  \dots, \vec{x}_n \in \mathbb{R}^2$ we may compute a series of unit vectors, $\vec{u}_k = \Delta \vec{x}_k / \| \Delta \vec{x}_k  \|$,
where $\Delta \vec{x}_k = \vec{x}_{k+1} - \vec{x}_k$.
A director-field may be learned from these directions by maximizing
$$
	R(\alpha ) = \sum_{k} \frac{x_{k+1} + x_k}{2} \cos( \theta_\alpha(\vec{x}_k) ) + \frac{y_{k+1} + y_k}{2}  \sin( \theta_\alpha( \vec{x}_k ) ).
$$
In words, this reward function measures the alignment of the director-field with an observation (i.e. the dot product), and takes sum over all observations.
An example of a learned director field is depicted in figure \ref{fig:director field}

%\begin{figure}[b] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=3in]{./figures/director_field.pdf} 
%   \caption{A director field learned from the trajectories (blue)}
%   \label{fig:director field}
%\end{figure}

\subsection{Online computations}
At run time, our goal is to compute a probability density, $P_T$ at time $T>0$ which correspond to the positions of the agents, given a measurement of position and velocity,  $\mu_0$ and $\eta_0$, at time $t=0$.
In more precise terms, we desire to compute
\begin{equation}
\begin{split}
&P(x_T \mid \eta_0, \mu_0 ) = \\
&\left( \sum_{ k=1}^n \int_{s \in S} ds P(x_T \mid \eta_0, \mu_0, c_k , s ) \cdot P( c_k , s \mid \eta_0, \mu_0 ) \right) \\
&+ P(x_T \mid \eta_0, \mu_0, c_0 ) \cdot P( c_0 \mid \eta_0, \mu_0 ) \label{eq:prob at T}
\end{split}
\end{equation}

%\left( \sum_{ k=1}^n \int_{s \in S} ds P(x_T \mid x_0, \eta_0, \mu_0, c_k , s ) \cdot P( c_k , s, x_0 \mid \eta_0, \mu_0 ) \right) 

\begin{thm} \label{thm:FP}
	Let $\rho_k(t,x)$ solve the continuity equation
	\begin{align*}
		\partial_t \rho_{k} + \sum_{\alpha} \partial_\alpha (  \rho_k \cdot X^\alpha_{k} ) = 0
	\end{align*}
	over the time interval $[0,T]$ with the initial condition $\rho_k(0,x) = P(x \mid c_k , \mu_0 , \eta_0 , s=1)$.
	Then $P(x_T \mid \eta_0 , \mu_0 )$ is given by
	\begin{align*}
		&\left( \sum_{k=1}^n \int_{s > 0} \rho_k( x , st ) P( c_k , s \mid \eta_0, \mu_0 )ds \right) \\
		&+ P(x_T \mid \eta_0, \mu_0, c_0 ) \cdot P( c_0 \mid \eta_0, \mu_0 )
	\end{align*}
\end{thm}

The proof of this theorem may be found in Appendix \ref{app:Fokker Planck}.

Theorem \ref{thm:FP} reduce the computation of \eqref{eq:prob at T} to the computation of solutions to a linear PDE
and the computation of a finite number of conditional probabilities at time $t=0$ (these are given in Appendix \ref{app:prob}).
We can approximate the solution of this linear PDE numerically, with a known error bound.
Such a numerical solution boils down to solving the ode, $\dot{x} = A \cdot x$, for a sparse matrix $A$.


\section{IMPLEMENTATION DETAILS}

\subsection{Data}
We use the Stanford Drone dataset (SDD) [THE CITATION SEEMS TO BE MISSING]

\subsection{Clustering}
Rather than training on all the data it is useful to cluster many of the trajectories.
The reason for this is.

\subsection{Training}


\section{EXPIREMENTS}
Evaluating the performance of a density estimator is more complex than evaluating the performance of a point predictor because we do not have direct access to a ground truth.
In this case, we have the locations of agents at times $t \in [0,T]$, but our method outputs a (smooth) probability density at times $t \in [0,T]$ given a probability density $P(x_0 \mid \mu_0 )$ at time $t=0$ (in our case, a Gaussian).
However, we can interpret the agent locations at time $t=0$ as samples from a Gaussian by assigning a weight to each location.
In particular, given trajectories $x_1(t),\dots, x_N(t)$ 
we can assume there exists an unknown distribution, $f$,
which produces the initial conditions $x_1(0), \dots, x_N(0)$.
We can define the weights
\begin{align*}
	w_i(\mu_0) := \frac{ P( x_i(0)  \mid \mu_0 ) }{ f_N (x_i(0)) }.
\end{align*}
where $f_N$ is an estimate of $f$ (e.g. obtained from kernel density estimation).
If $f_N$ converges weakly to $f$ as $N \to \infty$,
then the distribution
\begin{align*}
	q_0(x) := \sum_{i=1}^N w_i K_h( x - x_i(0) )
\end{align*}
converges (weakly) to $P(x \mid \mu_0)$ on the support of $f$
as $h \to 0$ and $N \to \infty$.
\todo[inline]{Still need to do this}

We can then consider the density
\begin{align*}
	q_t(x) := \sum_{i=1}^N w_i K_h( x - x_i(t) ).
\end{align*}
which can be compared with our predictor.
This induces the error estimate
\begin{align*}
	 e = \int_{0}^T \| q_t - p_{pred,t} \|_{L^1} dt
\end{align*}
We define the accuracy as $e^{-1}$ and the precision as the reciprocal of the total observed variation in $e$.
Because a single error estimate uses the entire test-set, we must use $k$-fold cross validation in order to get $k > 1$ error estimates.
We will restrict ourselves to scenes with more than 100 trajectories and let $k=10$.

\begin{enumerate}
	\item Accuracy = expected value of prediction error
	\item Precision = reciprocal of variance of prediction error
	\item CPU time = time to compute trajectory/density
\end{enumerate}

\begin{tabular}{|c|c|c|}
	\hline
	- & Linear & Us \\
	\hline
	Precision & - & - \\
	\hline
	Accuracy & - & - \\
	\hline
	error & - & - \\
	\hline
	CPU time & - & - \\
	\hline
\end{tabular}

\section{CONCLUSIONS}

A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

\subsection{Probability computations} \label{app:prob}
\todo[inline]{This section as been re-written from scratch every day for five days, and resulted in a different code each time.  It's the primary obstacle to implementation at the moment.}
The goal of this section is to put \eqref{eq:prob at T} into a tractable form for numerical computation.
We denote measurements of position by $\mu_0$ and we denote the ``true'' position by $x_0$.
Similarly, we denote a speed measurement by $\eta_0$ and we denote the ``true'' speed by $v_0$.
We choose a Gaussian noise model for the measurements:
\begin{align*}
	P( x_0 \mid \mu_0) = P( \mu_0 \mid x_0 ) = (2\pi \sigma_x^2)^{-1/2} \exp \left( \frac{ -\| x_0 - \mu_0 \|^2 }{ 2 \sigma_x^2 } \right) \\
	P( v_0 \mid \eta_0) = P( \eta_0 \mid v_0 ) = (2\pi \sigma_v^2)^{-1/2} \exp \left( \frac{ -\| v_0 - \eta_0 \|^2 }{ 2 \sigma_v^2 } \right)
\end{align*}
and we assume that $v_0$ is independent of $\mu_0$ given $\eta_0$.
Similarly, we assume that $x_0$ is independent of $\eta_0$ given $\mu_0$.
Thus $P( x_0, v_0 \mid \mu_0, \eta_0 ) = P(x_0 \mid \mu_0 ) P( v_0 \mid \eta_0)$.
These assumptions are enough to compute the conditional probabilities in \eqref{eq:prob at T}.

There are two components to \eqref{eq:prob at T}.
There is the $c_1,\dots,c_k$ component, and there is the linear predictor component, $c_0$.
Let's first focus on the $c_1,\dots,c_k$ component for a fixed $k$ and speed, $s$.

By Bayes' theorem
\begin{equation}
\begin{split}
	&P(x_T \mid \eta_0, \mu_0, c_k, s) = \\
	&\quad \int P(x_T \mid x_0, \eta_0, \mu_0, c_k, s) P( x_0 \mid \eta_0, \mu_0, c_k, x_0) dx_0.
\end{split}
\end{equation}
As $c_k$ and $s$ determine the dynamics completely and $\mu_0$ and $\eta_0$ contribute no new information when $x_0$ is given, we can write this as.
\begin{align*}
	&P(x_T \mid \eta_0, \mu_0, c_k, s) =\\
	&\quad \int P(x_T \mid x_0, c_k, s) P( x_0 \mid \eta_0, \mu_0, c_k, s) dx_0.
\end{align*}
Taking a derivative with respect to $T$ illustrates that $P(x_T \mid \eta_0 , \mu_0, c_k, s)$ can be seen as the solution of the Fokker-Planck equation at time $t=T$
with the initial condition $P(x_0 \mid \eta_0, \mu_0, c_k, s)$.
Thus we should derive a tractable expression for $P(x_0 \mid \eta_0, \mu_0, c_k, s)$.
We will assume that the true position, $x_0$, is independent of $\eta_0,c_k,s$ given the a measurement $\mu_0$.
Thus $P(x_0 \mid \eta_0, \mu_0, c_k, s) = P(x_0 \mid \mu_0)$ which is just a Gaussian centered at $\mu_0$.
We will refrain from improving upon this, although it is conceivable since $P(x \mid c_k)$ is precomputed and $\eta_0$ is more likely to be measured when it is close to $s X_k(x_0)$.


We also need to compute $P(c_k,s \mid \mu_0, \eta_0 )$.
By Bayes' theorem
\begin{align*}
	& P(c_k,s \mid \mu_0 , \eta_0) \\
	& = \int P(c_k,s \mid x_0, v_0, \mu_0 , \eta_0) P(x_0, v_0 \mid \mu_0, \eta_0 ) dx_0 dv_0 
\end{align*}
Assuming the measurements yield no additional information given the true position and velocity, we can assume $P(c_k,s \mid x_0, v_0, \mu_0 , \eta_0) = P(c_k,s \mid x_0, v_0)$.
The latter distribution is Dirac-delta like, and only activated when $s X_{c_k}(x_0) = v_0$.
Moreover, by assumption $P(x_0,v_0 \mid \mu_0, \eta_0) = P(x_0 \mid \mu_0) P(v_0 \mid \eta_0)$ is just a product a Gaussian distributions.
This yields the result
\begin{align*}
	&P(c_k,s \mid \mu_0 , \eta_0) = \\
	&\frac{1}{Z_{\mu_0,\eta_0} } \int  \exp \left( \frac{ - \| x_0 - \mu_0 \| }{ 2 \sigma_x^2 } + \frac{ - \| sX_{c_k}(x_0) - \eta_0 \| }{2 \sigma_v^2 } \right) dx_0
\end{align*}

In order to compute the normalizing constant we must know $P(c_0 \mid \mu_0, \eta_0)$.
Again, we must make a choice.  Such a choice determines how often we will use the linear predictor.
We'd like to use the linear predictor when a measured position is far from the training set (spatially)
or if a measured velocity is mis-aligned with all the director fields at the measured position.
This inspires the following choice
\begin{align*}
	& P(c_0 \mid \mu_0, \eta_0) = \\
	&\quad \prod_{k=1}^n \left[ 1 - \left\| \frac{ \eta_0 \cdot X_{k}(\mu_0) }{ \epsilon + \| \eta_0 \|} \right\|^{\gamma_a} \tanh \left( \gamma_x e^{ - V_{c_k}(\mu_0) } \right)  \right]
\end{align*}
where $\gamma_x, \gamma_a, \epsilon > 0$ are a tunable hyper-parameters.
When $\gamma_x$ is large or when $\gamma_a$ is small we will tend to rely on the linear predictor more often.
$\gamma_a$ pertains to how much we care about alignment between the measured velocity, and the velocity of the given class, while $\gamma_x$ pertains to how close we like to be to the test set when using a one of the classes we've learned.
The hyper-parameter, $\epsilon$, is a regularization term.  The alignment between $X_{k}$ and $\eta_0$ is given by $\eta_0 \cdot X_{k}(\mu_0) / \| \eta_0 \|$.  This alignment measure is undefined when $\eta_0 = 0$ so the parameter $\epsilon > 0$ is included to regularize this singularity.  What this means in practice is that when the measured velocity is small, the predictor will default to a linear predictor.
This completes the discussion for $c \in \{c_1,\dots, c_n \}$.

For the linear predictor case ($c=c_0$) we compute
\begin{align*}
	&P(x_T \mid \eta_0 , \mu_0, c_0 )  \\
	&= \int P(x_T \mid x_0, v_0, \mu_0, \eta_0, c_0) P( x_0, v_0 \mid \mu_0, \eta_0, c_0 ) dx_0 dv_0 \\
	&= \int \delta( x_T -  (x_0 + Tv_0) ) P( x_0, v_0 \mid \mu_0, \eta_0, c_0 ) dx_0 dv_0 \\
	&= \int P(x_0 = x_T - Tv_0 , v_0 \mid \mu_0 , \eta_0, c_0 ) dv_0 \\
	&= \int P(x_0 = x_T- Tv_0 \mid \mu_0 ) P( v_0 \mid \eta_0 ) dv_0
\end{align*}

If $P(v_0 \mid \eta_0) \sim \mathcal{N}( \eta_0, \sigma_v )$ then the above integral is merely a Gaussian convolution with respect to the variable $v_0$.
We can compute in closed form
\begin{align*}
		&= \int P(x_0 = x_T- Tv_0 \mid \mu_0 ) P( v_0 \mid \eta_0 ) dv_0 \\
		&= \frac{1}{ 2\pi (\sigma_x^2 + T^2 \sigma_v^2 ) } \exp \left( \frac{- \| x_T - T \eta_0 - \mu_0 \|^2 }{ 2(\sigma_x^2 + T^2 \sigma_v^2 )} \right)
\end{align*}

\section*{Fokker Planck} \label{app:Fokker Planck}
Proof = cuz math.

\section*{ACKNOWLEDGMENT}

The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


\bibliographystyle{IEEEtrans}
\bibliography{hoj}




\end{document}
