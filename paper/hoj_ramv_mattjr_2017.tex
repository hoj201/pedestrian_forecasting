%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{todonotes}

\newtheorem{thm}{Theorem}

\title{\LARGE \bf
Real-time Forecasting of Pedestrian Probability Densities*
}


\author{Henry O. Jacobs, Matt Johnson-Roberson, and Ram Vasudevan$^{2}$% <-this % stops a space
\thanks{*This work was supported by ???}% <-this % stops a space
\thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This electronic document is a “live” template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

blah blah blah

\section{PREVIOUS WORK}

\subsection{Social Force Models}
Cite these papers \cite{Henderson1971,Helbing1992}

\subsection{MDP Models}
Cite these papers \cite{Kitani2012,Ballan2016,Karasev2016,Ziebart2008}

\section{DATA EXPLORATION}
Simply viewing some of the plots we can make some qualitative observations regarding trajectories.
We can use these observations to simplify our analysis of the data and build our prediction algorithm.

\subsection{Speed is piecewise constant}
	The speed of the agents appears to be constant over the course of a few seconds.
	Changes is speed occur abruptly, shifting from one constant to another.
	A typical plot of speed is displayed in figure \ref{fig:speed}.

\begin{figure}[b] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.5\textwidth]{./figures/speed_plot.pdf} 
   \caption{We observe that speed appears to be a sum of step functions with widths lasting a few seconds}
   \label{fig:speed}
\end{figure}

\subsection{There is substantial spatial structure}
We see that the vast majority of trajectories appear to fall under a small number of clusters.
A plot of all trajectories from one video is depicted in figure \ref{fig:trajectories}.
Later, we will exploit this structure by using affinity propagation to cluster the trajectories and produce a
finite set of classes of motion for each scene.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=3in]{./figures/trajectories.pdf} 
   \caption{A plot of trajectories from one scene in the Stanford drone data-set}
   \label{fig:trajectories}
\end{figure}

\section{MODEL DESCRIPTION}
We consider a model where each agent belongs to an unobserved class from the set $C = \{c_0,c_1,\dots,c_n\}$.
Classes $c_1$ through $c_n$ are each associated with vector fields $X_1, \dots, X_n$ which determine the dynamics of the agent, modulo the observed speed.
Class $c_0$ corresponds to a linear predictor (i.e. moving with constant velocity).

Training entails learning the classes $c_1,\dots,c_n$ and the vector-field $X_1, \dots, X_n$ from observations of agent motion within a fixed scene.
We also learn the posterior probability of an agent being in a location given its class, as well as the prior probabilities for each class.
We view this learning step as a pre-computation.
At runtime we infer the probability of the class of an observed agent using it's position data.
This induces a probability distribution at time $t=0$ which we can evolve in time according to the dynamics of the vector-fields $X_1,\dots,X_n$.

\subsection{Offline computations}
Offline, the primary task is to learn the classes for the agents.
This entails clustering observed trajectories, and constructing a vector-field for each cluster.

\subsubsection{Clustering}
For a fixed scene with a database of agent trajectories we cluster the trajectories by applying the Affinity propagation algorithm to the end-points.
We then prune the clusters by discarding trajectories which are outliers with respect to total length (we define an outlier using the standard inter-quartile range criterion with a IQR coefficient of $1.5$).
We then throw out clusters which contain less than $10\%$ of the trajectories.
We associate class labels, $c_1,\dots,c_n$,  to the remaining clusters, and we will develop a model for each of these classes in the next section.
We also add an additional class, $c_0$, where the underlying model will be a linear predictor.
Finally, we define a prior, $P(c)$ to compute the probability that a given agent falls within one of these classes.
We set $P(c_0) = ( \text{\# discarded trajectories} / \text{ \# trajectories } )$ and $P(c_k ) = ( \text{\# trajectories in cluster $c_k$} / \text{ \# trajectories } )$ for $k=1,\dots,n$.

\subsubsection{Compute the posterior of class given a position}
Given an agent located at $x \in D$, we would like to know the conditional probability
that this agent is of class $c$. That is, we'd like to compute $P(c \mid x)$.
Moreover, during runtime we will need this computation to be fast.
This rules out standard probabilistic classification schemes such as support vector-machines and other local voting based methods.
The CPU time to evaluate grows with the amount of data in these methods.

Instead we uses a probability density inspired by statistical mechanics.
First, we use Baye's theorem to convert the computation into choosing Likelihood $P( x \mid c)$.
For the linear-predictor class, $c_0$ we assume $P(x \mid c_0 )$ is a uniform distribution over our domain.

For classes $c_1, \dots, c_n$ we use the following strategy.
Given points $x_{1,k},\dots,x_{N,k}$ associated to class $c_k$ we define
\begin{align}
	P(x \mid c_k) = \frac{1}{Z_k } e^{-V_k(x) } \label{eq:x given c}
\end{align}
where $V_k(x)$ is the minimizer of
$$
	C[ V_k ] =  \log \left( \int e^{ - V_k(x) } dx \right) + \sum_{i=1}^N V_k(x_{i,k} ).
$$
over some finite-dimensional function space (perhaps a space of low order polynomials).
Physically, $V_k(x)$ is the most likely potential function given observations $\{ x_{i,k}\}_{k=1}^N$ under the assumption that points are drawn from \eqref{eq:x given c}.

Again, the advantage to this approach is that we may restrict $V_c$ to a class of quickly computable functions which will not slow down performance at runtime.
In this paper we restrict $V_c$ to be a sum of tensor products of low order Legendre polynomials.

\begin{figure}[t] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{./figures/learned_potentials.pdf} 
   \caption{Some learned potentials for computing $P(x|c_k)$}
   \label{fig:learned potentials}
\end{figure}

\subsubsection{Learning vector fields}
Given trajectories, we may fit a director field of the form $X(x,y) = ( \cos( \theta_\alpha(x,y) ) , \sin( \theta_\alpha(x,y) ) )$
where 
$$
	\theta_\alpha(x,y) = \sum_{ij} \alpha_{ij} L_i(x / w) L_j(y / h).
$$


Given observations $\vec{x}_0,  \dots, \vec{x}_n \in \mathbb{R}^2$ we may compute a series of unit vectors, $\vec{u}_k = \Delta \vec{x}_k / \| \Delta \vec{x}_k  \|$,
where $\Delta \vec{x}_k = \vec{x}_{k+1} - \vec{x}_k$.
A director-field may be learned from these directions by maximizing
$$
	R(\alpha ) = \sum_{k} \frac{x_{k+1} + x_k}{2} \cos( \theta_\alpha(\vec{x}_k) ) + \frac{y_{k+1} + y_k}{2}  \sin( \theta_\alpha( \vec{x}_k ) ).
$$
In words, this reward function measures the alignment of the director-field with an observation (i.e. the dot product), and takes sum over all observations.
An example of a learned director field is depicted in figure \ref{fig:director field}

\begin{figure}[b] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=3in]{./figures/director_field.pdf} 
   \caption{A director field learned from the trajectories (blue)}
   \label{fig:director field}
\end{figure}

\subsection{Online computations}
At run time, our goal is to compute a probability density, $P_T$ at time $T>0$ which correspond to the positions of the agents, given a measurement of position and velocity,  $\mu_0$ and $\eta_0$, at time $t=0$.
In more precise terms, we desire to compute
\begin{equation}
\begin{split}
&P(x_T \mid \eta_0, \mu_0 ) = \\
&\left( \sum_{ k=1}^n \int_{s \in S} ds P(x_T \mid \eta_0, \mu_0, c_k , s ) \cdot P( c_k , s \mid \eta_0, \mu_0 ) \right) \\
&+ P(x_T \mid \eta_0, \mu_0, c_0 ) \cdot P( c_0 \mid \eta_0, \mu_0 ) \label{eq:prob at T}
\end{split}
\end{equation}

%\left( \sum_{ k=1}^n \int_{s \in S} ds P(x_T \mid x_0, \eta_0, \mu_0, c_k , s ) \cdot P( c_k , s, x_0 \mid \eta_0, \mu_0 ) \right) 

\begin{thm} \label{thm:FP}
	Let $\rho_k^{+}(t,x)$ solve the Fokker-Planck equation
	\begin{align*}
		\partial_t \rho_k^{+}  + \partial_\alpha (  \rho_k^+ X^\alpha_{c_k} ) = \frac{\sigma^2}{2} \Delta \rho_k^+
	\end{align*}
	and similarly, let $\rho_k^-$ satisfy
	\begin{align*}
		\partial_t \rho_k^{-}  - \partial_\alpha (  \rho_k^- X^\alpha_{c_k} ) = \frac{\sigma^2}{2} \Delta \rho_k^-
	\end{align*}
	over the time interval $[0,T]$.
	Moreover, assume the initial condition $\rho^{\pm}(0,x) = P(x \mid c_k , \mu_0 , \eta_0 , s=\pm1)$.
	Then $P(x_T \mid \eta_0 , \mu_0 )$ is given by
	\begin{align*}
		&\left( \sum_{k=1}^n \int_{s > 0} \rho_k^+( x , st ) P( c_k , s \mid \eta_0, \mu_0 )ds \right) \\
		&\left( \sum_{k=1}^n \int_{s > 0 } \rho_k^-( x , st ) P( c_k , - s \mid \eta_0, \mu_0 ) ds \right) \\
		&+ P(x_T \mid \eta_0, \mu_0, c_0 ) \cdot P( c_0 \mid \eta_0, \mu_0 )
	\end{align*}
\end{thm}

The proof of this theorem may be found in Appendix \ref{app:Fokker Planck}.

Theorem \ref{thm:FP} reduce the computation of \eqref{eq:prob at T} to the computation of solutions to a linear PDE
and the computation of a finite number of conditional probabilities at time $t=0$ (these are given in Appendix \ref{app:prob}).
We can approximate the solution of this linear PDE numerically, with a known error bound.
Such a numerical solution boils down to solving the ode, $\dot{x} = A \cdot x$, for a sparse matrix $A$.

\todo[inline]{
EXISTING ISSUE:
Still coding the various probabilities in Appendix.  As is, we are not computing many of them fast enough (CPU times in the seconds).  Some of them are throwing low-accuracy warnings too and failing to compute at all.
}

\section{IMPLEMENTATION DETAILS}

\subsection{Data}
We use the Stanford Drone dataset (SDD) [THE CITATION SEEMS TO BE MISSING]

\subsection{Clustering}
Rather than training on all the data it is useful to cluster many of the trajectories.
The reason for this is.

\subsection{Training}


\section{EXPIREMENTS}

\begin{enumerate}
	\item Accuracy = percentage of time prediction is within a 95\% confidence neighborhood
	\item Precision = size of 95\% confidence neighborhood
	\item CPU time = time to compute trajectory/density
	\item $L^1$ error of density. (we replace observation with a Gaussian)
\end{enumerate}

\begin{tabular}{|c|c|c|}
	\hline
	- & Linear & Us \\
	\hline
	Precision & - & - \\
	\hline
	Accuracy & - & - \\
	\hline
	error & - & - \\
	\hline
	CPU time & - & - \\
	\hline
\end{tabular}

\section{CONCLUSIONS}

A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

\subsection{Probability computations} \label{app:prob}
\todo[inline]{This section as been re-written from scratch every day for five days, and resulted in a different code each time.  It's the primary obstacle to implementation at the moment.}
The goal of this section is to put \eqref{eq:prob at T} into a tractable form for numerical computation.
We denote measurements of position by $\mu_0$ and we denote the ``true'' position by $x_0$.
Similarly, we denote a speed measurement by $\eta_0$ and we denote the ``true'' speed by $v_0$.
We choose a Gaussian noise model for the measurements:
\begin{align*}
	P( x_0 \mid \mu_0) = P( \mu_0 \mid x_0 ) = (2\pi \sigma_x^2)^{-1/2} \exp \left( \frac{ -\| x_0 - \mu_0 \|^2 }{ 2 \sigma_x^2 } \right) \\
	P( v_0 \mid \eta_0) = P( \eta_0 \mid v_0 ) = (2\pi \sigma_v^2)^{-1/2} \exp \left( \frac{ -\| v_0 - \eta_0 \|^2 }{ 2 \sigma_v^2 } \right)
\end{align*}
and we assume that $v_0$ is independent of $\mu_0$ given $\eta_0$.
Similarly, we assume that $x_0$ is independent of $\eta_0$ given $\mu_0$.
Thus $P( x_0, v_0 \mid \mu_0, \eta_0 ) = P(x_0 \mid \mu_0 ) P( v_0 \mid \eta_0)$.
These assumptions are enough to compute the conditional probabilities in \eqref{eq:prob at T}.

There are two components to \eqref{eq:prob at T}.
There is the $c_1,\dots,c_k$ component, and there is the linear predictor component, $c_0$.
Let's first focus on the $c_1,\dots,c_k$ component for a fixed $k$ and speed, $s$.

By Bayes' theorem
\begin{equation}
\begin{split}
	&P(x_T \mid \eta_0, \mu_0, c_k, s) = \\
	&\quad \int P(x_T \mid x_0, \eta_0, \mu_0, c_k, s) P( x_0 \mid \eta_0, \mu_0, c_k, x) dx_0.
\end{split}
\end{equation}
As $c_k$ and $s$ determine the dynamics completely and $\mu_0$ and $\eta_0$ contribute no new information when $x_0$ is given, we can write this as.
\begin{align*}
	&P(x_T \mid \eta_0, \mu_0, c_k, s) =\\
	&\quad \int P(x_T \mid x_0, c_k, s) P( x_0 \mid \eta_0, \mu_0, c_k, s) dx_0.
\end{align*}
Taking a derivative with respect to $T$ illustrates that $P(x_T \mid \eta_0 , \mu_0, c_k, s)$ can be seen as the solution of the Fokker-Planck equation at time $t=T$
with the initial condition $P(x_0 \mid \eta_0, \mu_0, c_k, s)$.
Thus we should derive a tractable expression for $P(x_0 \mid \eta_0, \mu_0, c_k, s)$.

We will assume, the symmetry
\begin{align*}
	P( x_0 \mid \mu_0 , \eta_0 , c_k , s ) = P( x_0 \mid \mu_0 , \gamma \eta_0 , c_k , \gamma s )
\end{align*}
for arbitrary $\gamma \in \mathbb{R}$.
Thus we may focus on the case where $s = 1$.
The primary bit of information one can glean on the location of $x_0$ given these measurements comes from the alignment between $X_c$ and the normalized vector $\hat{\eta}_0 = \eta_0 / \| \eta_0 \|$.
The magnitude, $\| \eta_0 \|$ and the speed lend no further information.
Thus $P( x_0 \mid \mu_0 , \eta_0 , c_k , s ) = P( x_0 \mid \mu_0 , \hat{\eta}_0 , c_k , s=1 )$.
We expect this to be high when $x_0$ is close to $\mu_0$ and $X_c(x_0)$ is close to $\hat{\eta}_0$.
We can't disintegrate this into conditionals we've already decided upon, so we introduce a new choice.
\begin{align*}
	&P(x_0 \mid \eta_0 ,\mu_0, c_k, s ) :=\\
	& \frac{1}{Z_{c_k,\mu_0,\hat{\eta}_0 }} \exp \left( \frac{ - \| X_{c_k} (x_0) - \hat{\eta_0} \| }{ 2 \sigma_v^2} + \frac{- \| x_0 - \mu_0 \| }{2 \sigma_x^2 } \right) 
\end{align*}

%To compute $P(x_0 \mid c_k, \mu_0)$ we would like to use our known expressions for $P(x_0 \mid c_k)$ (see \eqref{eq:x given c}) and the Gaussian measurement model.
%These two probability are sufficient to compute $P(x_0 \mid c_k,\mu_0)$ assuming that $c_k$ and $\mu_0$ are independent given $x_0$.
%Such an assumption is reasonable given that our measuring device existed long before our model did.
%Indeed
%\begin{equation}
%\begin{split}
%	& P(x_0 \mid c_k , \mu_0) \propto P(c_k, \mu_0 \mid x_0 ) P(x_0) \\
%	&=  P( \mu_0 \mid x_0) P( c_k \mid x_0 ) P(x_0)
%\end{split} \label{eq: x given c and mu}
%\end{equation}
%We can compute $P(c_k \mid x_0)$ from Bayes' theorem and \eqref{eq:x given c}.
%By our measurement mode, $P( \mu_0 \mid x_0)$ is normally distributed about $x_0$ with variance $\sigma_m$.
%Moreover, $P(x_0) = \sum_{c} P(x_0 \mid c) P(c)$
%where $P(c)$ is learned from the data and $P(x_0 \mid c)$ is given in \eqref{eq:x given c}.

We also need to compute $P(c_k,s \mid \mu_0, \eta_0 )$.
By Bayes' theorem
\begin{align*}
	& P(c_k,s \mid \mu_0 , \eta_0) \\
	& = \int P(c_k,s \mid x_0, v_0, \mu_0 , \eta_0) P(x_0, v_0 \mid \mu_0, \eta_0 ) dx_0 dv_0 
\end{align*}
Assuming the measurements yield no additional information given the true position and velocity, we can assume $P(c_k,s \mid x_0, v_0, \mu_0 , \eta_0) = P(c_k,s \mid x_0, v_0)$.
The latter distribution is Dirac-delta like, and only activated when $s X_{c_k}(x_0) = v_0$.
Moreover, by assumption $P(x_0,v_0 \mid \mu_0, \eta_0) = P(x_0 \mid \mu_0) P(v_0 \mid \eta_0)$ is just a product a Gaussian distributions.
This yields the result
\begin{align*}
	&P(c_k,s \mid \mu_0 , \eta_0) = \\
	&\frac{1}{Z_{\mu_0,\eta_0} } \int  \exp \left( \frac{ - \| x_0 - \mu_0 \| }{ 2 \sigma_x^2 } + \frac{ - \| sX_{c_k}(x_0) - \eta_0 \| }{2 \sigma_v^2 } \right) dx_0
\end{align*}

In order to compute the normalizing constant we must know $P(c_0 \mid \mu_0, \eta_0)$.
Again, we must make a choice.  Such a choice determines how often we will use the linear predictor.
We'd like to use the linear predictor when a measured position is far from the training set (spatially)
or if a measured velocity is mis-aligned with all the director fields at the measured position.
This inspires the following choice
\begin{align*}
	& P(c_0 \mid \mu_0, \eta_0) = \\
	&\quad \prod_{k=1}^n \left[ 1 - | \cos \left( \hat{\eta}_0 \cdot X_{c_k}(\mu_0) \right) |^{\gamma_a} \tanh \left( \gamma_x e^{ - V_{c_k}(\mu_0) } \right)  \right]
\end{align*}
where $\gamma_x > 0$ and $\gamma_a > 0$ are a tunable hyper-parameters.
When $\gamma_x$ is large or when $\gamma_a$ is small we will tend to rely on the linear predictor more often.
$\gamma_a$ pertains to how much we care about alignment between the measured velocity, and the velocity of the given class, while $\gamma_x$ pertains to how close we like to be to the test set when using a one of the classes we've learned.
This completes the discussion for $c \in \{c_1,\dots, c_n \}$.

For the linear predictor case ($c=c_0$) we compute
\begin{align*}
	&P(x_T \mid \eta_0 , \mu_0, c_0 )  \\
	&= \int P(x_T \mid x_0, v_0, \mu_0, \eta_0, c_0) P( x_0, v_0 \mid \mu_0, \eta_0, c_0 ) dx_0 dv_0 \\
	&= \int \delta( x_T -  (x_0 + Tv_0) ) P( x_0, v_0 \mid \mu_0, \eta_0, c_0 ) dx_0 dv_0 \\
	&= \int P(x_0 = x_T - Tv_0 , v_0 \mid \mu_0 , \eta_0, c_0 ) dv_0 \\
	&= \int P(x_0 = x_T- Tv_0 \mid \mu_0 ) P( v_0 \mid \eta_0 ) dv_0
\end{align*}

If $P(v_0 \mid \eta_0) \sim \mathcal{N}( \eta_0, \sigma_v )$ then the above integral is merely a Gaussian convolution with respect to the variable $v_0$.
We can compute in closed form
\begin{align*}
		&= \int P(x_0 = x_T- Tv_0 \mid \mu_0 ) P( v_0 \mid \eta_0 ) dv_0 \\
		&= \frac{1}{ 2\pi (\sigma_x^2 + T^2 \sigma_v^2 ) } \exp \left( \frac{- \| x_T - T \eta_0 - \mu_0 \|^2 }{ 2(\sigma_x^2 + T^2 \sigma_v^2 )} \right)
\end{align*}

\section*{Fokker Planck} \label{app:Fokker Planck}
Proof = cuz math.

\section*{ACKNOWLEDGMENT}

The preferred spelling of the word “acknowledgment” in America is without an “e” after the “g”. Avoid the stilted expression, “One of us (R. B. G.) thanks . . .”  Instead, try “R. B. G. thanks”. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


\bibliographystyle{IEEEtrans}
\bibliography{hoj}




\end{document}
