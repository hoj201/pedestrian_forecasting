{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression formulation\n",
    "Assuming a 1D Lagragian of the form $L_\\theta(x,y,u,v) = \\frac{1}{2} (u^2+v^2)- \\sum_{k_1,k_2} \\theta_{k_1,k_2} x^{k_1} y^{k_2}$ the EL-equations are given by\n",
    "$$\n",
    "    \\ddot{x} = - \\sum_{k_1=1,k_2=0}^{N,N} \\theta_{k_1,k_2} k_1 x^{k_1-1} y^{k_2} \\\\\n",
    "    \\ddot{y} = - \\sum_{k_1=0,k_2=1}^{N,N} \\theta_{k_1,k_2} k_2 x^{k_1} y^{k_2-1}\n",
    "$$   \n",
    "\n",
    "This inspires the cost function $C = C^x + C^y$ where\n",
    "$$\n",
    "    C^x(\\theta ) = \\frac{1}{2} \\sum_i \\left( \\ddot{x}_i + \\sum_{k_1=1,k_2=0}^{N,N} \\theta_{k_1,k_2} k_1 x_i^{k_1-1} y_i^{k_2} \\right)^2 \\\\\n",
    "    C^y(\\theta ) = \\frac{1}{2} \\sum_i \\left( \\ddot{y}_i + \\sum_{k_1=1,k_2=0}^{N,N} \\theta_{k_1,k_2} k_2 x_i^{k_1} y_i^{k_2-1} \\right)^2\n",
    "$$\n",
    "\n",
    "Taking the gradient we find\n",
    "$\\partial C / \\partial \\theta_{k_1,k_2} = \\partial C^x / \\partial \\theta_{k_1,k_2} + \\partial C^y / \\partial \\theta_{k_1,k_2} $ with\n",
    "$$\n",
    "    \\frac{\\partial C^x}{\\partial \\theta_{j_1,j_2} } = \\sum_i \\left( \\ddot{x}_i + \\sum_{k_1=1,k_2=0}^{N,N} \\theta_{k_1,k_2} k_1 x_i^{k_1-1} y_i^{k_2} \\right) x^{j_1-1} y^{j_2}\\\\\n",
    "    \\frac{\\partial C^y}{\\partial \\theta_{j_1,j_2} } = \\sum_i \\left( \\ddot{y}_i + \\sum_{k_1=1,k_2=0}^{N,N} \\theta_{k_1,k_2} k_2 x_i^{k_1} y_i^{k_2-1} \\right) x^{j_1} y^{j_2-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear regression formulation\n",
    "Again, we assume a Lagrangian of the form $L_\\theta(x,y,u,v) = \\frac{1}{2} (u^2 + v^2) - V(x,y;\\theta)$ with\n",
    "$$\n",
    "    V(x,y;\\theta) = \\sum_{k_1,k_2} \\theta_{k_1,k_2} x^{k_1} y^{k_2}.\n",
    "$$\n",
    "Given observations $\\{ (q_i(0),\\dot{q}_i(0) , q_i(1) ) \\}_i$We consider the cost function\n",
    "$$\n",
    "    Q(\\theta) = \\sum_{i} \\| q_i(1) - \\hat{q}_i(1) \\|_{L^1}\n",
    "$$\n",
    "Where $\\hat{q}_i(1)$ is obtained from $q_i(0)$ and $\\dot{q}_i(0)$ by solving the Euler-Lagrange equations \n",
    "\n",
    "The gradient of $Q$ is\n",
    "$$\n",
    "    \\frac{ \\partial Q}{\\partial \\theta} =\n",
    "       - \\sum_i {\\rm sign}( x_i(1) - \\hat{x}_i(1) ) \\frac{ \\partial \\hat{x}_i(1) }{\\partial \\theta}\n",
    "       + {\\rm sign}( y_i(1) - \\hat{y}_i(1) ) \\frac{ \\partial \\hat{y}_i(1) }{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Explicitly, we get $\\partial_\\theta \\hat{x}(1)$ and $\\partial_\\theta \\hat{y}(1)$ from solving the ODE\n",
    "$$\n",
    "    \\frac{dx}{dt} = u \\\\\n",
    "    \\frac{dy}{dt} = v \\\\\n",
    "    \\frac{du}{dt} = - \\sum_{k_1=1,k_2=0} \\theta_{k_1,k_2} k_1 x^{k_1-1} y^{k_2} \\\\\n",
    "    \\frac{dv}{dt} = - \\sum_{k_1=0,k_2=1} \\theta_{k_1,k_2} k_2 x^{k_1} y^{k_2-1} \\\\\n",
    "    \\frac{d}{dt} \\left( \\frac{\\partial x}{\\partial \\theta_{k_1,k_2}} \\right) = \\frac{\\partial u}{\\partial \\theta_{k_1,k_2} } \\\\\n",
    "    \\frac{d}{dt} \\left( \\frac{\\partial y}{\\partial \\theta_{k_1,k_2}} \\right) = \\frac{\\partial v}{\\partial \\theta_{k_1,k_2} } \\\\\n",
    "    \\frac{d}{dt} \\left( \\frac{\\partial u}{\\partial \\theta_{k_1,k_2}} \\right) = - k_1 x^{k_1-1} y^{k_2} \\\\\n",
    "    \\frac{d }{dt} \\left( \\frac{\\partial v}{\\partial \\theta_{k_1,k_2}} \\right) = - k_2 x^{k_1} y^{k_2-1}\n",
    "$$\n",
    "with the initial condition $\\delta q(0) = 0$ and $\\delta \\dot{q}(0) = 0$\n",
    "We can use stochastic gradient descent to minimize $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=5\n",
    "\n",
    "def state_to_vars(s):\n",
    "    # returns variables x,y,u,v,delta_x,delta_y,delta_u,delta_v\n",
    "    global N\n",
    "    x = s[0]\n",
    "    y = s[1]\n",
    "    u = s[2]\n",
    "    v = s[3]\n",
    "    N_sq = N*N\n",
    "    r = np.arange(4,N_sq+4)\n",
    "    delta_x = s[r].reshape((N,N))\n",
    "    delta_y = s[r+N_sq].reshape((N,N))\n",
    "    delta_u = s[r+2*N_sq].reshape((N,N))\n",
    "    delta_v = s[r+3*N_sq].reshape((N,N))\n",
    "    return x,y,u,v,delta_x,delta_y,delta_u,delta_v\n",
    "\n",
    "def vars_to_state(x,y,u,v,delta_x,delta_y,delta_u,delta_v):\n",
    "    # returns a state\n",
    "    f = lambda x: list( x.flatten() )\n",
    "    return np.array([x,y,u,v] + f(delta_x) + f(delta_y) + f(delta_u) + f(delta_v))\n",
    "\n",
    "def ode_func(theta,s):\n",
    "    # a function which can be input into odeint for a fixed theta\n",
    "    global N\n",
    "    x,y,u,v,delta_x,delta_y,delta_u,delta_v = state_to_vars(s)\n",
    "    x_dot = u\n",
    "    y_dot = v\n",
    "    pow_x = np.vander([x],N=N,increasing=True)[0] #pow_x[j] = x^j\n",
    "    pow_y = np.vander([y],N=N,increasing=True)[0]\n",
    "    u_dot = - np.einsum('ij,i,i,j',theta[1:N,0:N], np.arange(1,N),pow_x[0:N-1],pow_y[0:N])\n",
    "    v_dot = - np.einsum('ij,j,i,j',theta[0:N,1:N], np.arange(1,N),pow_x[0:N],pow_y[0:N-1])\n",
    "    delta_x_dot = delta_u\n",
    "    delta_y_dot = delta_v\n",
    "    delta_u_dot = np.zeros_like(theta)\n",
    "    delta_u_dot[1:N,0:N] = - np.einsum( 'i,i,j->ij',np.arange(1,N) , pow_x[0:N-1], pow_y )\n",
    "    delta_v_dot = np.zeros_like(theta)\n",
    "    delta_v_dot[0:N,1:N] = - np.einsum( 'j,i,j->ij',np.arange(1,N) , pow_x, pow_y[0:N-1] )\n",
    "    s_out = vars_to_state(x_dot,y_dot,u_dot,v_dot,\\\n",
    "                          delta_x_dot,delta_y_dot,\\\n",
    "                          delta_u_dot,delta_v_dot)\n",
    "    return s_out\n",
    "    \n",
    "    \n",
    "def partial_Q(theta,x0,y0,u0,v0,x1,y1):\n",
    "    #produces the Q_i and the gradient of Q_i(\\theta,x)\n",
    "    global N\n",
    "    from scipy.integrate import odeint\n",
    "    s0 = np.zeros(4+4*N*N)\n",
    "    s0[0:4] = np.array([x0,y0,u0,v0])\n",
    "    s1 = odeint( lambda s,t: ode_func(theta,s) , s0 , [0.0,0.1], rtol=0.01)[1]\n",
    "    x,y,u,v,delta_x,delta_y,delta_u,delta_v = state_to_vars(s1)\n",
    "    Q = x\n",
    "    dQ = delta_x\n",
    "    return Q,dQ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the Gradient:\n",
      "-0.00114560798625\n",
      "-0.00139262155152\n"
     ]
    }
   ],
   "source": [
    "print \"Testing the Gradient:\"\n",
    "theta = np.random.randn(N,N)\n",
    "x0,y0,u0,v0,x1,y1 = list(np.random.randn(6))\n",
    "Q,dQ = partial_Q(theta,x0,y0,u0,v0,x1,y1)\n",
    "theta_perturbed = theta.copy()\n",
    "h = 0.00001\n",
    "i = 2\n",
    "j = 4\n",
    "theta_perturbed[i,j] += h\n",
    "P,dP = partial_Q(theta_perturbed,x0,y0,u0,v0,x1,y1)\n",
    "print (P - Q)/(h)\n",
    "print (dQ[i,j] + dP[i,j])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training\n",
    "Now we need to implement Stochastic Gradient Descent (SGD).  The basic idea is to increment $\\theta$ by\n",
    "$\\theta \\mapsto \\theta - \\eta \\nabla Q_i(\\theta)$\n",
    "over some dataset $\\{ (x0,y0,u0,v0,x1,y1)_i \\}_{i=1}^N$\n",
    "Let us choose randomly sample $1000$ data-points for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
