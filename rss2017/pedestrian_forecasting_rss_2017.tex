\documentclass[conference]{IEEEtran}
\usepackage{times}

\usepackage{amsmath,amssymb}

%  For drawing Bayesian networks
\usepackage{tikz}
\usetikzlibrary{arrows,positioning}
\usepackage{tikz-cd}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Hal 9000)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{Realtime Probabilistic Pedestrian Forecasting}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
The abstract goes here.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

\section{Model}
%Describe scenario in english.
Our goal is to generate a time-dependent probability density over $\mathbb{R}^2$, which predicts the true location of an agent in the future.
The input to the algorithm at runtime is a noisy measurement of position and velocity, $\hat{x}_0, \hat{v}_0 \in \mathbb{R}^2$.
If the (unknown) location of agent at time $t$ is given by $x_t \in \mathbb{R}^2$, then the distribution we seek is the posterior $\rho_t(x_t) := \Pr( x_t \mid \hat{x}_0, \hat{v}_0 )$.

In order to derive an expression for $\rho$ we will build a probabilistic graphical model.
Our model has two parts.
First, we specify how measurements of position and velocity, $\hat{x}_0$ and $\hat{v}_0$, are related to the true position and velocity, $x_0$ and $v_0 := \left. \frac{d}{dt} \right|_{t=0} x_t$.
This determines posteriors such as $\Pr(\hat{x}_0 \mid x_0)$.
Secondly, we must know how the true initial conditions, $x_0$ and $v_0$ determine the final condition of $x_t$.
This determines posteriors like $\Pr(x_t \mid x_0, v_0, f )$, where $f$ refers to other parameters which will be introduced later.

It is a simple exercise in probability to derive
\begin{align*}
	\rho_t(x_t) &= \Pr( x_t \mid \hat{x}_0, \hat{v}_0 ) \\
	&\propto \Pr( x_t, \hat{x}_0, \hat{v}_0 ) \\
	&= \int \Pr( x_t \mid  x_0, v_0, f ) \\
	 &\qquad \Pr( \hat{x}_0  \mid x_0 ) \Pr( \hat{v}_0 \mid v_0 ) dx_0 \, dv_0 \, df
\end{align*}

In the next few subsections we will describe the posteriors used in the above calculation.

\subsection{The sensor model}
At time $t=0$, we obtain a noisy reading of position, $\hat{x}_0 \in \mathbb{R}^2$.
We assume that our measurements come from Gaussian noise about the true location $x_0 \in \mathbb{R}^2$,
so that $\Pr(\hat{x}_0 \mid x_0 ) = (2\pi \sigma_x^2)^{-1} \exp( - \| \hat{x}_0 - x_0 \|^2 / (2 \sigma_x^2) )$.
Similarly, we have a velocity reading $\hat{v}_0 \in \mathbb{R}^2$ which is conditionally dependent on the true velocity $v_0 \in \mathbb{R}^2$
via the posterior $\Pr( \hat{v}_0 \mid v_0 ) = (2\pi \sigma_v^2)^{-1} \exp( - \| \hat{v}_0 - v_0 \|^2 / (2 \sigma_v^2) )$.
We should note that $\hat{v}_0$ is typically derived from multiple measurements of position via a finite difference.
This induces a linear inequality between $\sigma_x$ and $\sigma_v$.
For example, a first order finite difference would yield $\sigma_v \geq 2 \sigma_x / \Delta t$ where $\Delta t > 0$ is the time-step size between consecutive measurements of position.

\subsection{The motion model}
All agents are initialized within some rectangular region $D \subset \mathbb{R}^2$.
Once initialized, agents come in two flavors: linear and nonlinear.
Linear agents move in straight lines at constant velocity.
Thus $x_t = x_0 + t v_0$ and so we have the posterior
\begin{align*}
	\Pr( x_t \mid x_0, v_0, lin) = \delta( x_t - x_0 - t v_0 ).
\end{align*}
We will assume linear agents also satisfy the posteriors
\begin{align*}
	\Pr( x_0 \mid lin ) \sim \mathcal{U}( D)\quad,\quad \Pr( v_0 \mid lin ) \sim \mathcal{N}( 0 , \sigma_L).
\end{align*}

If the agent is of nonlinear type, then we assume the dynamics take the form
\begin{align}
	\frac{dx_t}{dt} \equiv v_t = s \cdot X_k(x_t) \label{eq:ode}
\end{align}
where $X_k$ is a vector-field from a finite collection $\{X_1, \dots, X_n\}$, and $s \in \mathbb{R}$.
This collection is learned from the data-set.
It is assumed that $k$ and $s$ are both constant in time, so that the position $x_t$ is determined from the
tripe $(x_0,k,s)$ by integrating \eqref{eq:ode}.
In other words, we have the posterior
\begin{align*}
	\Pr( x_t \mid x_0 , k , s) = \delta( x_t - \Phi^{t}_{k,s}(x_0) )
\end{align*}
where $\Phi^{t}_{k,s}$ is the flow-map of the vector field $s \cdot X_k$ up to time $t$.
It is notable, that the variables $k,s$ and $x_0$ determine $v_0$.
Thus we also have the posterior
\begin{align*}
	\Pr( v_0 \mid k,s,x_0) = \delta( v_0 -s X_k(x_0) ).
\end{align*}
In summary, the agent flavors are parametrized by the set
\begin{align*}
	F = \{ lin \} \cup \left( \mathbb{R} \times \{ 1 , \dots, n \} \right)
\end{align*}
and each flavor determines the sort of motion we should expect from that agent.

\subsection{The full model}
Concatenating the measurement model with our motion model yields the Bayesian network
\begin{center}
\begin{tikzpicture}[thick, var/.style={circle,draw,thin,rounded corners,shade,top color=blue!50,minimum size = 4mm}]
	\node[var] (f) {$f$};
	\node[var] (x) [right=of f] {$x_0$};
	\node[var] (v) [below=of x] {$v_0$};
	\node[var] (x_hat) [right=of x] {$\hat{x}_0$};
	\node[var] (v_hat) [right=of v] {$\hat{v}_0$};
	\node[var] (x_t) [left=of v] {$x_t$};
	
	\draw[->] (f.east) to (x.west);
	\draw[->] (f.south east) to (v.north west);
	\draw[->] (f.south) to (x_t.north);
	\draw[->] (v.west) to (x_t.east);
	\draw[->] (x.south west) to (x_t.north east);
	\draw[->] (x.south) to (v.north);
	\draw[->] (x.east) to (x_hat.west);
	\draw[->] (v.east) to (v_hat.west);
\end{tikzpicture}
\end{center}
Which allows us to derive $\rho_t$ by appropriately marginalizing over the full joint probability $\Pr(x_t, x_0, v_0, \hat{x}_0, \hat{v}_0, f)$. [Reference]
Specifically, the vector-fields $\{X_1,\dots,X_n\}$ as well as the probabilities $\Pr( x_0 \mid k )$, $\Pr(lin)$, and $\Pr( k )$ will be learned from the data.
We will also assume $\Pr(s,k) = \Pr(s \mid k ) \Pr(k)$ where 
\begin{align*}
	\Pr(s \mid k ) \sim \mathcal{U}([-s_{\max}, s_{\max} ] ) \quad \text{for } k=1,\dots,n
\end{align*}
and $s_{\max} > 0$ is learned from the data.
We describe the learning step in more detail in section \ref{sec:learning}.

The desired posterior, $\rho_t$ is a posterior over $D$ given by
\begin{align*}
	&\rho_t(x_t) = \Pr(x_t \mid \hat{x}_0, \hat{v}_0) \\
		&= \frac{ \Pr( x_t, \hat{x}_0, \hat{v}_0) }{ \Pr( \hat{x}_0, \hat{v}_0 ) } \\
		&= \frac{1}{ \Pr( \hat{x}_0, \hat{v}_0 ) } \int \Pr( x_t , x_0, v_0, \hat{x}_0, \hat{v}_0 , f ) dx_0\, dv_0\, df
\end{align*}
and the integrand is given, via the Bayesian network.
In particular, for $f = (k,s)$ we have
\begin{align*}
	 &\Pr( x_t , x_0, v_0, \hat{x}_0, \hat{v}_0 , k,s) =\\
	 &\quad \Pr( x_t \mid x_0 , k, s) \cdot \Pr( v_0 \mid x_0, k , s ) \cdot \\
	 &\quad \Pr( \hat{x}_0 \mid x_0 ) \cdot \Pr( \hat{v}_0 \mid v_0 ) \cdot \Pr(k) \cdot \Pr(s \mid k),
\end{align*}
and for $f = lin$ we have
\begin{align*}
	 &\Pr( x_t , x_0, v_0, \hat{x}_0, \hat{v}_0 , lin) =\\
	 &\quad \Pr( x_t \mid x_0 , v_0, lin) \cdot \Pr( v_0 \mid x_0, lin ) \cdot \\
	 &\quad \Pr( \hat{x}_0 \mid x_0 ) \cdot \Pr( \hat{v}_0 \mid v_0 ) \cdot \Pr(lin).
\end{align*}

The abundance of Dirac deltas and Gaussians allows the integrals involved in computing $\rho_t$ to be solved in closed form
as smooth functions over $\mathbb{R}^2$.

\subsection{Inverse optimal control}
In this optional subsection, we illustrate how we can restrict the vector-fields $X_k$, so that the nonlinear agents move along trajectories which solve relavant optimal control problems.
In particular, if we force $X_k$ to be of unit magnitude, in that $\| X_k(x) \|_2 = 1$ for all $x \in \mathbb{R}^2$.
Then a solution $x(t)$ solves the optimal control problem
\begin{align*}
	
\end{align*}

\section{Learning} \label{sec:learning}
  Describe how $\Pr(x \mid k)$, the director-fields, and $\sigma_x$ are learned without using math.
  Math will be relegated to the appendix.
  
\section{Results}
 Precision, accuracy, etc.

\section{RSS citations}

Please make sure to include \verb!natbib.sty! and to use the
\verb!plainnat.bst! bibliography style. \verb!natbib! provides additional
citation commands, most usefully \verb!\citet!. For example, rather than the
awkward construction 

{\small
\begin{verbatim}
\cite{kalman1960new} demonstrated...
\end{verbatim}
}

\noindent
rendered as ``\cite{kalman1960new} demonstrated...,''
or the
inconvenient 

{\small
\begin{verbatim}
Kalman \cite{kalman1960new} 
demonstrated...
\end{verbatim}
}

\noindent
rendered as 
``Kalman \cite{kalman1960new} demonstrated...'', 
one can
write 

{\small
\begin{verbatim}
\citet{kalman1960new} demonstrated... 
\end{verbatim}
}
\noindent
which renders as ``\citet{kalman1960new} demonstrated...'' and is 
both easy to write and much easier to read.
  
\subsection{RSS Hyperlinks}

This year, we would like to use the ability of PDF viewers to interpret
hyperlinks, specifically to allow each reference in the bibliography to be a
link to an online version of the reference. 
As an example, if you were to cite ``Passive Dynamic Walking''
\cite{McGeer01041990}, the entry in the bibtex would read:

{\small
\begin{verbatim}
@article{McGeer01041990,
  author = {McGeer, Tad}, 
  title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}}, 
  volume = {9}, 
  number = {2}, 
  pages = {62-82}, 
  year = {1990}, 
  doi = {10.1177/027836499000900206}, 
  URL = {http://ijr.sagepub.com/content/9/2/62.abstract}, 
  eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html}, 
  journal = {The International Journal of Robotics Research}
}
\end{verbatim}
}
\noindent
and the entry in the compiled PDF would look like:

\def\tmplabel#1{[#1]}

\begin{enumerate}
\item[\tmplabel{1}] Tad McGeer. \href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic
Walking}. {\em The International Journal of Robotics Research}, 9(2):62--82,
1990.
\end{enumerate}
%
where the title of the article is a link that takes you to the article on IJRR's website. 


Linking cited articles will not always be possible, especially for
older articles. There are also often several versions of papers
online: authors are free to decide what to use as the link destination
yet we strongly encourage to link to archival or publisher sites
(such as IEEE Xplore or Sage Journals).  We encourage all authors to use this feature to
the extent possible.

\section{Conclusion} 
\label{sec:conclusion}

The conclusion goes here.

\section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


