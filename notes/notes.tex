\documentclass[12pt]{amsart}
\usepackage{amsmath,amssymb}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage{tikz}
\usetikzlibrary{arrows,positioning}
\usepackage{tikz-cd}
\usepackage{algorithm2e}
%  POSSIBLY USEFULE PACKAGES
%\usepackage{graphicx}
%\usepackage{tensor}

%  NEW COMMANDS
\newcommand{\pder}[2]{\ensuremath{\frac{ \partial #1}{\partial #2}}}
\newcommand{\ppder}[3]{\ensuremath{\frac{\partial^2 #1}{\partial
      #2 \partial #3} } }

%  NEW THEOREM ENVIRONMENTS
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}


%  MATH OPERATORS
\DeclareMathOperator{\Diff}{Diff}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\Prob}{Prob}

%  TITLE, AUTHOR, DATE
\title{Notes}
\author{Henry O. Jacobs}
\date{\today}


\begin{document}

\maketitle

\section{Problem Description}
Given a finite set of time-series of bounding boxes, representing the location of previously observed agents, and
two successive bounding boxes of a newly observed agent, can we generate a time-dependent probability density, $\rho$, which represents the
newly observed agent's location at times $t \in [0, T]$?

\section{Our model}
We will assume that agents are of two types, linear or nonlinear.  Linear agents have dynamics specified by
\begin{align}
	\dot{x} (t)= x_0 + t \cdot v_0 \label{eq:lin}
\end{align}
where $v_0, x_0 \in \mathbb{R}^2$ are the position and velocity at time $t=0$.
Nonlinear agents come in flavors $\{1,\dots,n\}$ have dynamics given by
\begin{align}
	\dot{x} = s X_k(x) \label{eq:nonlin}
\end{align}
for some $k \in \{1,\dots,n\}$, a predetermined director-field, $X_k$, and some constant $s \in \mathbb{R}$.
This motion model allows us to decompose the computation of $\rho(t)$ into tractable components.
In particular $\rho(t,x)$ is defined as the probability density of finding the agent at position $x$ at time $t$ given measurements of position and velocity $\hat{x}_0, \hat{v}_0$ at time $t=0$.
In terms of the density at time $t=0$, $\rho$ is given by
\begin{align*}
	\rho(t,x ) := \Pr( x = x(t)\mid \hat{x}_0, \hat{v}_0 )
\end{align*}
If we know the agent is linear and we know the velocity, then we know the position $x(t)$ by \eqref{eq:lin}.
If we know the agent is nonlinear, and we know the variables $k$ and $s$, then we know the position $x(t)$ by integrating \eqref{eq:nonlin}.
These observations suggest that we decompose the computation of $\rho$ using Baye's theorem, as
We find
\begin{align*}
	\rho(t,x ) &:= \int \Pr( x = x(t) \mid x(0) = x_0, \mu ) \cdot \Pr(x_0 \mid \mu ) dx_0 \\
	&= \sum_{k=1}^{n} \int \Pr( x = x(t) \mid k,s, x(0) = x_0, \mu ) \cdot P( k,s,x_0 \mid \mu) ds \, dx_0 \\
	&\quad + \int \Pr(x = x(t) \mid \Lin, x(0) = x_0, v_0 , \mu ) P( \Lin, x_0, v_0  \mid \mu ) dx_0 dv_0
\end{align*}

There are a lot of terms to deal with here.
Let us first deal with $\Pr( x = x(t) \mid k,s, x(0) = x_0, \mu )$.
Given \eqref{eq:nonlin} determines $x(t)$ precisely given and agent's flavor, $k$, speed, $s$, and initial condition,
we should expect a Dirac-delta type distribution which is independent of $\mu$.
Moreover, if $\Phi_k^t$ is the time-$t$ flow of $X_k$, then $\Phi_k^{st}$ is the time $t$ flow of $s X_k$. 
This means
\begin{align*}
	\Pr( x  = x(t) \mid k,s, x_0 = x_0 , \mu ) = \delta( x - \Phi_k^{st}( x_0) ).
\end{align*}

Similarly,
\begin{align*}
	\Pr( x = x(t) \mid \Lin, x(0) = x_0, v_0 , \mu ) = \delta \left( x - (x_0 + t \cdot v_0 ) \right)
\end{align*}

Substitution, and the change of variables formula tells us that
\begin{align*}
	\rho(t,x) &= \sum_{k=1}^n \int \det \left[ (D\Phi_{k}^{st})^{-1}(x)  \right] \cdot \Pr( k, s, x_0 = (\Phi_{k}^{st})^{-1} (x) \mid \mu) ds \\
		&\quad + \int \Pr( x_0 = x- t v_0 ,v_0, \Lin \mid \mu)dv_0
\end{align*}

So if we can compute the flow $\Phi_k^{t}$ efficiently, and we can compute $\Pr( k,s,x \mid \hat{x}, \hat{v} )$ and $\Pr( x, v, \Lin \mid \hat{x}, \hat{v} )$ efficiently, then we are good to go.
Computing the flow can be done using standard ode integration schemes, so the main obstacle is computing the posteriors $\Pr( k,s,x \mid \hat{x}, \hat{v} )$ and $\Pr( x, v, \Lin \mid \hat{x}, \hat{v} )$.

\section{Choice of atomic distributions, and expressions for composite distributions}
In order to compute $\Pr( k,s,x \mid \hat{x}, \hat{v} )$ and $\Pr( x, v, \Lin \mid \hat{x}, \hat{v} )$ we use a probabilistic graphical model to organize our computations.
To begin, define the set
\begin{align*}
	Type = \{ \Lin \} \cup \{ (k,s) \mid s \in [-\bar{s}, \bar{s}] , k = 1,\dots,n\}.
\end{align*}
Each agent is associated with one and only one type.
The type of a given agents provides information about his true position $x$ based on how often we've see an agent of such a type at location $x$.
Moreover, given an agents true position, and his type, we know something about his/her velocity.
For example, if an agent is of type $(k,s)$ and is known to be at position $x$, then we know with certainty that his velocity is $v = sX_k(x)$.
Lastly, we assume our measurements of position are independent of Type, given the true position.
This yields the probabilistic graphical model
\begin{equation}
\begin{tikzcd}
	Type \arrow[r] \arrow[rd] & x \arrow[d] \arrow[r] & \hat{x} \\
	 & v \ar[r] & \hat{v} \\
\end{tikzcd}
\label{eq:pgm}
\end{equation}
Under this model, the posteriors $\Pr( k,s,x \mid \hat{x}, \hat{v} )$ and $\Pr( x, v, \Lin \mid \hat{x}, \hat{v} )$ can be computed in terms of the \emph{atomic distributions}
\begin{itemize}
	\item $\Pr( x \mid Type )$
	\item $\Pr( v \mid Type, x)$
	\item $\Pr( \hat{x} \mid x)$
	\item $\Pr( \hat{v} \mid v)$
	\item $\Pr(Type)$
\end{itemize}
These atomic distributions are chosen, and represent a set of parameters for our model.
In the remainder of this section, we will assume that these atomic densities are known, and easily computable, expressions.
We list precise choices for the atomic densities in the appendix.

To calculate $\Pr(k,s,x \mid \hat{x}, \hat{v} )$ in terms of the atomic distribution we use \eqref{eq:pgm} to find
\begin{align*}
	\Pr( k,s,x \mid \hat{x}, \hat{v}) &= \frac{ \Pr(k, s, x, \hat{x}, \hat{v} ) }{ \Pr( \hat{x}, \hat{v} ) } \\
		&= \frac{ \Pr(k,s,x, \hat{x}, \hat{v} ) }{ \Pr( \hat{x}, \hat{v}) } \\
		&= \frac{ \int \Pr(k, s, x, v, \hat{x}, \hat{v} ) dv }{ \Pr( \hat{x}, \hat{v} ) }
\end{align*}
Using the pgm diagram we can decompose the integrand in the numerator as
\begin{align*}
	&= \frac{1}{ \Pr( \hat{x}, \hat{v} ) } \int \Pr( x \mid k,s) \Pr( \hat{x} \mid x ) \Pr( \hat{v} \mid v) \Pr( v \mid k,s,x) \Pr(k,s) dv \\
	&= \frac{ \Pr( x \mid k,s) \Pr( \hat{x} \mid x ) \Pr( \hat{v} \mid v = s X_k(x) ) }{ \Pr( \hat{x} , \hat{v} ) }
\end{align*}
In order to compute $\Pr( \hat{x}, \hat{v})$ we can compute
\begin{align*}
	\Pr( \Lin , x \mid \hat{x} , \hat{v} ) = \frac{ \Pr( \Lin , x, \hat{x}, \hat{v} ) }{ \Pr( \hat{x}, \hat{v} ) } 
\end{align*}
where
\begin{align*}
	&\Pr( \Lin, x, \hat{x}, \hat{v} ) = \int \Pr( x \mid \Lin ) \Pr( \hat{x} \mid x ) \Pr( \hat{v} \mid v) \Pr( v \mid \Lin ,x) \Pr(\Lin) dv \\
	&\quad = \frac{ \Pr( \Lin ) \Pr( x \mid \Lin ) \Pr( \hat{x} \mid x ) }{ 2\pi \sigma_v^2 w_v^2} \int_{\hat{v}_1 - w_v/2}^{\hat{v}_1 + w_v/2}  \int_{\hat{v}_2 - w_v/2}^{\hat{v}_2 + w_v/2} \exp \left( \frac{ - (v_1^2 + v_2^2) }{ 2 \sigma_v^2} \right) dv_1 dv_2 \\
	&\quad = \frac{ \Pr( \Lin ) \Pr( x \mid \Lin ) \Pr( \hat{x} \mid x ) }{ 4 \sigma_v^2 w_v^2} \prod_{i=1,2} \left( \erf \left( \frac{ \hat{v}_i + w_v/2}{ \sigma \sqrt{2} } \right) - \erf \left( \frac{ \hat{v}_i - w_v/2}{ \sigma \sqrt{2} } \right) \right)
\end{align*}
Then we may use the fact that
\begin{align*}
	\Pr( \hat{x}, \hat{v} ) = \int \Pr( \Lin , x , \hat{x}, \hat{v} )dx  + \sum_{k} \int \Pr( k,s, x , \hat{x} , \hat{v} ) dx ds
\end{align*}

Finally, we should note that
\begin{align*}
	\Pr( \Lin , x, v \mid \hat{x}, \hat{v} ) = \frac{ \Pr( \Lin , x, v ,\hat{x}, \hat{v} ) }{ \Pr( \hat{x} , \hat{v} ) }
\end{align*}
where the numerator is
\begin{align*}
	\Pr( \Lin, x, v, \hat{x}, \hat{v} ) = \Pr( x \mid \Lin ) \Pr( \hat{x} \mid x ) \Pr( \hat{v} \mid v) \Pr( v \mid \Lin ,x) \Pr(\Lin).
\end{align*}



\section{Code design}

The ultimate goal is to compute $\rho(x,t)$, or at least to integrate over a region of space-time.
To do this we can consider the following diagram of modules

\begin{tikzpicture}[thick, module/.style={rectangle,draw,thin,rounded corners,shade,top color=blue!50,minimum size = 4mm}]
	\node[module] (Atomic) {Atomic distributions};
	\node[module] (ML) [above=of Atomic]{Machine Learning};
	\node[module] (IC) [right=of Atomic] { Composite distributions };
	\node[module] (rho) [right=of IC] {$\rho(x,t)$};
	\node[module] (quad) [below=of IC] {Quadrature};
	\node[module] (ode) [above=of IC] {Ode integration};
	
	\draw[->] (Atomic.east) to (IC.west);
	\draw[->] (ML.south) to node[auto]{scene object} (Atomic.north);
	\draw[->] (IC.east) to (rho.west);
	\draw[->] (quad.north) to (IC.south);
	\draw[->] (ode.east) to [out=0,in=90] (rho.north);
	\draw[->] (quad.east) to [out=0,in=270] (rho.south);
\end{tikzpicture}

Each arrow in the above diagram corresponds to an import or a load command.
The names of each module should convey what sort of computations the module is responsible for.
For example, the Machine learning module processes the data from the Stanford drone data set, and outputs a scene object.
This scene object includes variables $\theta, \alpha, P(k), P(s), P(\Lin)$ and the max bounding box width.
where $\theta$ is a 3d array which allows one to compute the director field for each class
and $\alpha$ is a 3d array which allows one to compute the potential function involved in the atomic distribution $\Pr( x_0 \mid k)$.
The usage of $\theta$ and $\alpha$ is described in appendix \ref{app:learning}.

\section{Precision and Recall}
We would like to view the computed $\rho$ as a classifier.
Let $\tau > 0$.
For any measurable set $E$, we classify it as positive at time $t > 0$ if $\int_E \rho(x,t)dx > \tau$.
The precision over the time interval $[0,T]$ with respect to the test-sets $\mathcal{E}$ is given by
\begin{align*}
	precision( \tau , \mathcal{E} ) = \inf_{t \in [0,T] } \frac{ \#\{ E \in \mathcal{E}^+(t) \mid \int_E \rho_t > \tau \} }{  \#\{ E \in \mathcal{E}(t) \mid \int_E \rho_t > \tau \} } \\
	recall( \tau , \mathcal{E} ) = \sup
\end{align*}

\appendix

\section{Atomic distributions}
Let $w_x, w_v  > 0$ be a bound for the width of the bounding boxes and bounding box velocities we observe in the data.
Similarly, let $V_k: \mathbb{R}^2 \to \mathbb{R}$ be an energy function and $X_k \in \mathfrak{X}( \mathbb{R}^2)$ a director field we learn from the data. (see Appendix \ref{app:learning} )
The atomic distributions, $\Pr(k)$, $\Pr(s)$, and $\Pr(\Lin)$ are actually computed in the Machine Learning module and then imported.
Here are posteriors:
\begin{align*}
	\Pr( \hat{x}_0 \mid x_0 ) &\sim \mathcal{U}( x_0 ; w_x ) \\
	\Pr( \hat{v}_0 \mid v_0 ) &\sim \mathcal{U}( v_0 ; w_v ) \\
	\Pr( x_0 \mid k,s ) &= \Pr(x_0 \mid k) = \frac{1}{Z_k} \exp \left( -V_k( x_0 ) \right) \\
	\Pr( v_0 \mid k,s,x_0) &= \delta( v_0 - s X_k(x_0) )
\end{align*}
The only remaining posterior to provide is $\Pr( x \mid \Lin )$ and $\Pr( v \mid x,\Lin )$.
We let $\Pr( x_0 \mid \Lin)$ be a uniform distribution over our domain and we let
\begin{align}
	\Pr( v_0 \mid x_0 , \Lin ) = \Pr( v_0 \mid \Lin ) \sim \mathcal{N}( 0, \sigma_{\Lin} ) \label{eq:v given x and Linear}
\end{align}
where $\sigma_{\Lin} > 0$ is learned from the data.

\section{The Machine Learning Module}
\label{app:learning}
The machine learning module is responsible for clustering the trajectories, and providing the vector-fields and potential functions for each cluster, as well as the probability $\Pr(k), \Pr(s)$ and $\Pr( \Lin)$ and the parameter $\sigma_{\Lin}$.

\subsubsection{Learning clusters}
For a fixed scene with a database of agent trajectories we cluster the trajectories by applying the Affinity propagation algorithm to the end-points.
We then prune the clusters by discarding trajectories which are outliers with respect to total length (we define an outlier using the standard inter-quartile range criterion with a IQR coefficient of $1.5$).
We then throw out clusters which contain less than $10\%$ of the trajectories.
We associate class labels, $1,\dots,n$,  to the remaining clusters, and we will develop a model for each of these classes in the next section.
We also add an additional class, ``$\Lin$'', where the underlying model will be a linear predictor.
Finally, we define a prior, $P(c)$ to compute the probability that a given agent falls within one of these classes.
We set 
\begin{align*}
	P(\Lin) &= \frac{ \text{\# discarded trajectories} }{ \text{ \# trajectories } } \\
	P(k) &= \frac{ \text{\# trajectories in cluster $k$} }{ \text{ \# trajectories } } \text{ for } k=1,\dots,n.
\end{align*}

\subsubsection{Learning $P(x \mid k)$ }
During runtime we will need this computation to be fast, and to scale with the size of the data-set.
This rules out standard probabilistic classification schemes such kernel density methods.
Instead we discretize the space of energy functions, and compute on a subspace of fixed dimension.
For the linear-predictor class, $\Lin$, we assume $P(x \mid \Lin )$ is a uniform distribution over our domain.
For $k = 1, \dots, n$ we use the data to learn $P(x \mid k)$.
Given data points $x_{1,k},\dots,x_{N,k}$ associated to class $k$ we define
\begin{align}
	P(x \mid k) := \frac{1}{Z_k } e^{-V_k(x) } \label{eq:x given c}
\end{align}
where
$$
	V_k =  \text{argmin}_{V \in H_n}  \left( \log \left( \int e^{ - V(x) } dx \right) + \sum_{i=1}^N V(x_{i,k} ) \right).
$$
over some finite-dimensional subspace, $H_n \subset C^0( \mathbb{R}^2)$ (perhaps a space of low order polynomials).
This is a justifiable choice, since $V_k(x)$ is the most likely potential function in $H_n$, if the observations $\{ x_{i,k}\}_{k=1}^N$ are drawn from \eqref{eq:x given c}.
Again, the advantage to this approach is that we may restrict $V_k$ to a class of quickly computable functions which will not slow down performance at runtime.
In my current implementation, $H_n$ is a sum of tensor products of low order Legendre polynomials.

\subsubsection{Learning vector fields}
Given trajectories, we may fit a director field of the form $X(x,y) = ( \cos( \theta_\alpha(x,y) ) , \sin( \theta_\alpha(x,y) ) )$
where 
$$
	\theta_\alpha(x,y) = \sum_{ij} \alpha_{ij} L_i(x / w) L_j(y / h).
$$


Given observations $\vec{x}_0,  \dots, \vec{x}_n \in \mathbb{R}^2$ we may compute a series of unit vectors, $\vec{u}_k = \Delta \vec{x}_k / \| \Delta \vec{x}_k  \|$,
where $\Delta \vec{x}_k = \vec{x}_{k+1} - \vec{x}_k$.
A director-field may be learned from these directions by maximizing
$$
	R(\alpha ) = \sum_{k} \frac{x_{k+1} + x_k}{2} \cos( \theta_\alpha(\vec{x}_k) ) + \frac{y_{k+1} + y_k}{2}  \sin( \theta_\alpha( \vec{x}_k ) ).
$$
In words, this reward function measures the alignment of the director-field with an observation (i.e. the dot product), and takes sum over all observations.

\section{Computation of $\rho(x,t)$}
Assuming we have reliable quadrature and ode integration routines, we should be able to compute $\rho(x,t)$ efficiently.
In particular,
\begin{align*}
	\rho(x,t) = \rho_{\Lin}(x,t) +  \sum_{k} \rho_k(x,t)
\end{align*}
where
\begin{align*}
	\rho_k(x,t) &= \int_{-s_{\max}}^{s_{\max}} \Pr( x_0 = (\Phi_k^{st})^{-1}(x) , k , s \mid \mu ) \det( D\Phi_k^{st}(x)^{-1} ) ds \\
		&= \int_{-s_{\max}}^{ s_{\max}} (\Phi_{k}^{st})_* \rho_{k,s}(x) ds.
\end{align*}
where $\rho_{k,s}(x) =  \Pr( x_0 =x , k , s \mid \mu )$.
For $t = j \cdot \Delta t$ we can use a Riemann sum to approximate the above quadrature as
\begin{align*}
	\rho_k(x, j \Delta t) = \sum_{\ell = -j}^{j} (\Phi_{k}^{ t_\ell } )_* \rho_{k,s_\ell}(x) \Delta s + \mathcal{O}( \Delta s ).
\end{align*}
where
\begin{align*}
	t_\ell &= s_{\max}\, \ell \, \Delta t \\
	\Delta s &= s_{\max} \, \Delta t \\
	s_\ell &= \ell \, s_{\max} / j.
\end{align*}

Of course, we can use higher order quadrature rule if we desire greater orders of accuracy in $s$.
The advantage of the above formula is that we may recycle our computation of the flow, $\Phi_k^{t_\ell}$, as $j$ increases.
The push-forward distributions cab be approximated using Riemann quadrature, as sums of Dirac delta distributions.
In particular
\begin{align*}
	\Phi_* \rho_{k,s} = \| \Delta \vec{x} \| \cdot \sum_{x \in \Gamma} \rho_{k,s}( x ) \delta_{ \Phi(x) } + \mathcal{O}( \| \Delta \vec{x} \| ).
\end{align*}
where $\Gamma$ is a regular grid, overlaid on the support of $\rho_{k,s}$ with a volume element $\| \Delta \vec{x} \|$.

This gives us the following pipeline for plotting $\rho_k(x,t)$ for $t \in 0, \Delta t, \dots, j \cdot \Delta t$.
\begin{algorithm}[H]
	\KwData{ scene, $\mu, N_t , \Delta t$ }
	\KwResult{ $\rho_k(t)$ for $t = 0, \Delta t, \dots, N_t \, \Delta t$ }
	Initialize $\Gamma_0$ and $V_0 = 1$ \;
	\For{$j=1,\dots, N_t$}{
		Compute $\Gamma_j^{+} , V_j^{+} = F_{k}( \Gamma_{j-1}^+, V_{j-1}^+ ; \Delta t )$ \;
		Compute $\Gamma_j^{- } , V_j^{- } = F_{k}( \Gamma_{j-1}^- , V_{j-1}^-  ; -\Delta t )$ \;
		Let $\rho_k =  \sum_{ x\in \Gamma_0} \Pr( x_0=x_\alpha , k , s=0 \mid \mu) \delta_{x}$ \;
		\For{$\ell = 1,\dots, j$}{
			Compute $p_\alpha^+ = \Pr( x_0=x_\alpha , k , s_\ell \mid \mu) \Delta s$ for $x_\alpha \in \Gamma_0$ \;
			$\rho_k += \sum_{\alpha} p_\alpha^+ \delta_{ x_{\alpha,\ell} }$ for $x_{\alpha,\ell} \in \Gamma_\ell^+$ \;
			Compute $p_\alpha^- = \Pr( x_0=x_\alpha , k , -s_\ell \mid \mu) \Delta s$ for $x_\alpha \in \Gamma_0$ \;
			$\rho_k += \sum_{\alpha} p_\alpha^- \delta_{ x_{\alpha,\ell} }$ for $x_{\alpha,\ell} \in \Gamma_\ell^-$ \;
		}
		\Return $\rho_k$
	}
\end{algorithm}


\section{Numerical analysis for our particle method}

Let $\rho_0 \in W^{1,1}( \mathbb{R}^n) $ with a support contained in a compact rectangular domain $D$ which we uniform ally grid.
By the Sobelev embedding theorem we know that $\rho_0 \in L^\infty$.
Let $\Gamma \subset \mathbb{R}^n$ be a regular grid on $D$ with resolution $\Delta \vec{x}$ and volume element $ \| \Delta \vec{x} \|$.
Then
\begin{align*}
	 \left \| \rho_0 - \| \Delta \vec{x} \| \cdot \sum_{x \in \Gamma} \rho_0( x ) \delta_{x} \right \|_{L^1} = \mathcal{O}( \| \Delta \vec{x} \| ).
\end{align*}

Therefore
\begin{align*}
	\Phi_* \rho_0 = \| \Delta \vec{x} \| \cdot \sum_{x \in \Gamma} \rho_0( x ) \delta_{ \Phi(x) } + \mathcal{O}( \| \Delta \vec{x} \| ).
\end{align*}
for any $C^1$ diffeomorphism $\Phi$.
\emph{Note: when you plot this density on your computer screen, it is tempting to just plot a bunch of dots at $\Phi(x)$ for $x \in \Gamma$,
weighted by $\rho_0(x)$, then linearly interpolate.
However, this is not what the eye expects (although it has the correct information).
When people ``draw a picture of a distribution'', $\mu$, they are not really drawing a distribution (i.e. an element of a dual space to $C^0)$.
They are typically drawing a Radon-Nikodym derivative with respect to the Lebesgue measure.
In this case, before plotting, we should divide the values of each point by the amount of expansion witnessed at that point, $\det( D\Phi(x) )$.
It's a subtle point, since there is a lot of human psychology going on here.
The plotted object is a manipulated version of the object under scrutiny.}




\bibliographystyle{amsalpha}
\bibliography{hoj.bib}
\end{document}
