\documentclass[12pt]{amsart}
\usepackage{amsmath,amssymb}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage{tikz}
\usetikzlibrary{arrows,positioning}
\usepackage{tikz-cd}
%  POSSIBLY USEFULE PACKAGES
%\usepackage{graphicx}
%\usepackage{tensor}

%  NEW COMMANDS
\newcommand{\pder}[2]{\ensuremath{\frac{ \partial #1}{\partial #2}}}
\newcommand{\ppder}[3]{\ensuremath{\frac{\partial^2 #1}{\partial
      #2 \partial #3} } }

%  NEW THEOREM ENVIRONMENTS
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}


%  MATH OPERATORS
\DeclareMathOperator{\Diff}{Diff}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\Prob}{Prob}

%  TITLE, AUTHOR, DATE
\title{Notes}
\author{Henry O. Jacobs}
\date{\today}


\begin{document}

\maketitle

\section{Problem Description}
Given a finite set of time-series of bounding boxes, representing the location of previously observed agents, and
two successive bounding boxes of a newly observed agent, can we generate a time-dependent probability density, $\rho$, which represents the
newly observed agent's location at times $t \in [0, T]$?

\section{Our model}
We will assume that agents are of two types, linear or nonlinear.  Linear agents have dynamics specified by
\begin{align}
	\dot{x} (t)= x_0 + t \cdot v_0 \label{eq:lin}
\end{align}
where $v_0, x_0 \in \mathbb{R}^2$ are the position and velocity at time $t=0$.
Nonlinear agents come in flavors $\{1,\dots,n\}$ have dynamics given by
\begin{align}
	\dot{x} = s X_k(x) \label{eq:nonlin}
\end{align}
for some $k \in \{1,\dots,n\}$, a predetermined director-field, $X_k$, and some constant $s \in \mathbb{R}$.
This motion model allows us to decompose the computation of $\rho(t)$ into tractable components.
In particular $\rho(t,x)$ is defined as the probability density of finding the agent at position $x$ at time $t$ given measurements of position and velocity $\mu = (\hat{x}_0, \hat{v}_0)$ at time $t=0$.
In terms of the density at time $t=0$, $\rho$ is given by
\begin{align*}
	\rho(t,x ) := \Pr( x = x(t)\mid \mu )
\end{align*}
If we know the agent is linear and we know the velocity, then we know the position $x(t)$ by \eqref{eq:lin}.
If we know the agent is nonlinear, and we know the variables $k$ and $s$, then we know the position $x(t)$ by integrating \eqref{eq:nonlin}.
These observations suggest that we decompose the computation of $\rho$ using Baye's theorem, as
We find
\begin{align*}
	\rho(t,x ) &:= \int \Pr( x = x(t) \mid x(0) = x_0, \mu ) \cdot \Pr(x_0 \mid \mu ) dx_0 \\
	&= \sum_{k=1}^{n} \int \Pr( x = x(t) \mid k,s, x(0) = x_0, \mu ) \cdot P( k,s,x_0 \mid \mu) ds \, dx_0 \\
	&\quad + \int \Pr(x = x(t) \mid \Lin, x(0) = x_0, v_0 , \mu ) P( \Lin, x_0, v_0  \mid \mu ) dx_0 dv_0
\end{align*}

There are a lot of terms to deal with here.
Let us first deal with $\Pr( x = x(t) \mid k,s, x(0) = x_0, \mu )$.
Given \eqref{eq:nonlin} determines $x(t)$ precisely given and agent's flavor, $k$, speed, $s$, and initial condition,
we should expect a Dirac-delta type distribution which is independent of $\mu$.
Moreover, if $\Phi_k^t$ is the time-$t$ flow of $X_k$, then $\Phi_k^{st}$ is the time $t$ flow of $s X_k$. 
This means
\begin{align*}
	\Pr( x  = x(t) \mid k,s, x_0 = x_0 , \mu ) = \delta( x - \Phi_k^{st}( x_0) ).
\end{align*}

Similarly,
\begin{align*}
	\Pr( x = x(t) \mid \Lin, x(0) = x_0, v_0 , \mu ) = \delta \left( x - (x_0 + t \cdot v_0 ) \right)
\end{align*}

Substitution, and the change of variables formula tells us that
\begin{align*}
	\rho(t,x) &= \sum_{k=1}^n \int \det \left[ (D\Phi_{k}^{st})^{-1}(x)  \right] \cdot \Pr( k, s, x_0 = (\Phi_{k}^{st})^{-1} (x) \mid \mu) ds \\
		&\quad + \int \Pr( x_0 = x- t v_0 ,v_0, \Lin \mid \mu) \Pr( \Lin, x_0 =x- t v_0, v_0 \mid \mu )dv_0
\end{align*}

So if we can compute the flow $\Phi_k^{t}$ efficiently, and we can compute $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$ efficiently, then we are good to go.
Computing the flow can be done using standard ode integration schemes, so the main obstacle is computing the posteriors $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$.

\section{Expressions for $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$}
In order to compute $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$ we use a probabilistic graphical model to organize our computations.
To begin, define the set
\begin{align*}
	Type = \{ \Lin \} \cup \{ (k,s) \mid s \in [-\bar{s}, \bar{s}] , k = 1,\dots,n\}.
\end{align*}
Each agent is associated with one and only one type.
The type of a given agents provides information about his true position $x_0$ based on how often we've see an agent of such a type at location $x_0$.
Moreover, given an agents true position, and his type, we know something about his/her velocity.
For example, if an agent is of type $(k,s)$ and is known to be at position $x_0$, then we know with certainty that his velocity is $v_0 = sX_k(x_0)$.
Lastly, we assume our measurements of position are independent of Type, given the true position.
This yields the probabilistic graphical model
\begin{equation}
\begin{tikzcd}
	Type \arrow[r] \arrow[rd] & x_0 \arrow[d] \arrow[r] & \hat{x}_0 \\
	 & v_0 \ar[r] & \hat{v}_0 \\
\end{tikzcd}
\label{eq:pgm}
\end{equation}
Under this model, the posteriors $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$ can be computed in terms of the \emph{atomic densities}
\begin{itemize}
	\item $\Pr( x_0 \mid Type )$
	\item $\Pr( v_0 \mid Type,x_0)$
	\item $\Pr( \hat{x}_0 \mid x_0)$
	\item $\Pr( \hat{v}_0 \mid v_0)$
	\item $\Pr(Type)$
\end{itemize}
In the remainder of this section, we will assume that these atomic densities are known, and easily computable, expressions.
We list precise choices for the atomic densities in the appendix.

To calculate $\Pr(k,s,x_0 \mid \mu )$ in terms of the atomic densities we use \eqref{eq:pgm} to find
\begin{align*}
	\Pr( k,s,x_0 \mid \mu ) &\propto \Pr( \mu \mid k,s,x_0) \Pr( k,s,x_0) \\
		&= \Pr( \hat{x}_0 \mid x_0) \left(  \int \Pr( \hat{v}_0 \mid v_0 ) \Pr( v_0 \mid k,s,x_0) dv_0 \right) \Pr(k,s,x_0) \\
		&=  \Pr( \hat{x}_0 \mid x_0) \left(  \int \Pr( \hat{v}_0 \mid v_0 ) \Pr( v_0 \mid k,s,x_0) dv_0 \right) \Pr(x_0 \mid k) \Pr(k) \Pr(s) \\
		&=  \Pr( \hat{x}_0 \mid x_0) \Pr( \hat{v}_0 \mid v_0 = sX_k(x_0) ) \Pr(x_0 \mid k) \Pr(k) \Pr(s)
\end{align*}
Each of the posteriors in the last line is assumed known.

Similarly, in the linear case
\begin{align*}
	\Pr( \Lin , x_0, v_0 \mid \hat{x}_0, \hat{v}_0 ) &\propto \Pr( \hat{x}_0, \hat{v}_0 \mid \Lin, x_0, v_0 ) \Pr( \Lin, x_0, v_0 ) \\
	&= \Pr( \hat{x}_0, \hat{v}_0 \mid \Lin, x_0, v_0 ) \Pr( x_0, v_0 \mid \Lin ) \Pr(\Lin) \\
	&= \Pr( \hat{x}_0 \mid x_0 ) \Pr( \hat{v}_0 \mid v_0 ) \Pr( x_0 \mid \Lin ) \Pr( v_0 \mid x_0, \Lin ) \Pr(\Lin)
\end{align*}
Each of the above posteriors is assumed known.


\section{Code design}

The ultimate goal is to compute $\rho(x,t)$, or at least to integrate over a region of space-time.
To do this we can consider the following diagram of modules

\begin{tikzpicture}[thick, module/.style={rectangle,draw,thin,rounded corners,shade,top color=blue!50,minimum size = 4mm}]
	\node[module] (Atomic) {Atomic distributions};
	\node[module] (ML) [above=of Atomic]{Machine Learning};
	\node[module] (IC) [right=of Atomic] { $\Pr( x_0,k,s \mid \mu)$ and $\Pr( x_0, v_0 , \Lin \mid \mu)$ };
	\node[module] (rho) [right=of IC] {$\rho(x,t)$};
	\node[module] (quad) [below=of IC] {Quadrature};
	\node[module] (ode) [above=of IC] {Ode integration};
	
	\draw[->] (Atomic.east) to (IC.west);
	\draw[->] (ML.south) to (Atomic.north);
	\draw[->] (IC.east) to (rho.west);
	\draw[->] (quad.north) to (IC.south);
	\draw[->] (ode.east) to [out=0,in=90] (rho.north);
	\draw[->] (quad.east) to [out=0,in=270] (rho.south);
\end{tikzpicture}

Each arrow in the above diagram corresponds to an import command.
The names of each module should convey what sort of computations the module is responsible for.
For example, the Machine learning module processes the data from the Stanford drone data set, and outputs data that are needed in order to compute the atomic posteriors.

\appendix

\section{Atomic distributions}
Let $w_x, w_v  > 0$ be a bound for the width of the bounding boxes and bounding box velocities we observe in the data.
Similarly, let $V_k: \mathbb{R}^2 \to \mathbb{R}$ be an energy function and $X_k \in \mathfrak{X}( \mathbb{R}^2)$ a director field we learn from the data. (see Appendix \ref{app:learning} )
The atomic distributions, $\Pr(k)$, $\Pr(s)$, and $\Pr(\Lin)$ are actually computed in the Machine Learning module and then imported.
Here are posteriors:
\begin{align*}
	\Pr( \hat{x}_0 \mid x_0 ) &\sim \mathcal{U}( x_0 ; w_x ) \\
	\Pr( \hat{v}_0 \mid v_0 ) &\sim \mathcal{U}( v_0 ; w_v ) \\
	\Pr( x_0 \mid k,s ) &= \Pr(x_0 \mid k) = \frac{1}{Z_k} \exp \left( -V_k( x_0 ) \right) \\
	\Pr( v_0 \mid k,s,x_0) &= \delta( v_0 - s X_k(x_0) )
\end{align*}
The only remaining posterior to provide is $\Pr( x \mid \Lin )$ and $\Pr( v \mid x,\Lin )$.
We let $\Pr( x_0 \mid \Lin)$ be a uniform distribution over our domain and we let
\begin{align}
	\Pr( v_0 \mid x_0 , \Lin ) \sim \mathcal{N}( x_0, \sigma_{\Lin} ) \label{eq:v given x and Linear}
\end{align}
where $\sigma_{\Lin} > 0$ is learned from the data.

\section{The Machine Learning Module}
\label{app:learning}
The machine learning module is responsible for clustering the trajectories, and providing the vector-fields and potential functions for each cluster, as well as the probability $\Pr(k), \Pr(s)$ and $\Pr( \Lin)$ and the parameter $\sigma_{\Lin}$.

\subsubsection{Learning clusters}
For a fixed scene with a database of agent trajectories we cluster the trajectories by applying the Affinity propagation algorithm to the end-points.
We then prune the clusters by discarding trajectories which are outliers with respect to total length (we define an outlier using the standard inter-quartile range criterion with a IQR coefficient of $1.5$).
We then throw out clusters which contain less than $10\%$ of the trajectories.
We associate class labels, $1,\dots,n$,  to the remaining clusters, and we will develop a model for each of these classes in the next section.
We also add an additional class, ``$\Lin$'', where the underlying model will be a linear predictor.
Finally, we define a prior, $P(c)$ to compute the probability that a given agent falls within one of these classes.
We set 
\begin{align*}
	P(\Lin) &= \frac{ \text{\# discarded trajectories} }{ \text{ \# trajectories } } \\
	P(k) &= \frac{ \text{\# trajectories in cluster $k$} }{ \text{ \# trajectories } } \text{ for } k=1,\dots,n.
\end{align*}

\subsubsection{Learning $P(x \mid k)$ }
During runtime we will need this computation to be fast, and to scale with the size of the data-set.
This rules out standard probabilistic classification schemes such kernel density methods.
Instead we discretize the space of energy functions, and compute on a subspace of fixed dimension.
For the linear-predictor class, $\Lin$, we assume $P(x \mid \Lin )$ is a uniform distribution over our domain.
For $k = 1, \dots, n$ we use the data to learn $P(x \mid k)$.
Given data points $x_{1,k},\dots,x_{N,k}$ associated to class $k$ we define
\begin{align}
	P(x \mid k) := \frac{1}{Z_k } e^{-V_k(x) } \label{eq:x given c}
\end{align}
where
$$
	V_k =  \text{argmin}_{V \in H_n}  \left( \log \left( \int e^{ - V(x) } dx \right) + \sum_{i=1}^N V(x_{i,k} ) \right).
$$
over some finite-dimensional subspace, $H_n \subset C^0( \mathbb{R}^2)$ (perhaps a space of low order polynomials).
This is a justifiable choice, since $V_k(x)$ is the most likely potential function in $H_n$, if the observations $\{ x_{i,k}\}_{k=1}^N$ are drawn from \eqref{eq:x given c}.
Again, the advantage to this approach is that we may restrict $V_k$ to a class of quickly computable functions which will not slow down performance at runtime.
In my current implementation, $H_n$ is a sum of tensor products of low order Legendre polynomials.

\subsubsection{Learning vector fields}
Given trajectories, we may fit a director field of the form $X(x,y) = ( \cos( \theta_\alpha(x,y) ) , \sin( \theta_\alpha(x,y) ) )$
where 
$$
	\theta_\alpha(x,y) = \sum_{ij} \alpha_{ij} L_i(x / w) L_j(y / h).
$$


Given observations $\vec{x}_0,  \dots, \vec{x}_n \in \mathbb{R}^2$ we may compute a series of unit vectors, $\vec{u}_k = \Delta \vec{x}_k / \| \Delta \vec{x}_k  \|$,
where $\Delta \vec{x}_k = \vec{x}_{k+1} - \vec{x}_k$.
A director-field may be learned from these directions by maximizing
$$
	R(\alpha ) = \sum_{k} \frac{x_{k+1} + x_k}{2} \cos( \theta_\alpha(\vec{x}_k) ) + \frac{y_{k+1} + y_k}{2}  \sin( \theta_\alpha( \vec{x}_k ) ).
$$
In words, this reward function measures the alignment of the director-field with an observation (i.e. the dot product), and takes sum over all observations.

\bibliographystyle{amsalpha}
\bibliography{hoj.bib}
\end{document}
