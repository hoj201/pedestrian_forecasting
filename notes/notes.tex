\documentclass[12pt]{amsart}
\usepackage{amsmath,amssymb}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

\usepackage{tikz-cd}
%  POSSIBLY USEFULE PACKAGES
%\usepackage{graphicx}
%\usepackage{tensor}
\usepackage{todonotes}

%  NEW COMMANDS
\newcommand{\pder}[2]{\ensuremath{\frac{ \partial #1}{\partial #2}}}
\newcommand{\ppder}[3]{\ensuremath{\frac{\partial^2 #1}{\partial
      #2 \partial #3} } }

%  NEW THEOREM ENVIRONMENTS
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}


%  MATH OPERATORS
\DeclareMathOperator{\Diff}{Diff}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\Prob}{Prob}

%  TITLE, AUTHOR, DATE
\title{Notes}
\author{Henry O. Jacobs}
\date{\today}


\begin{document}

\maketitle

\section{Problem Description}
Given a finite set of time-series of bounding boxes, representing the location of previously observed agents, and
two successive bounding boxes of a newly observed agent, can we generate a time-dependent probability density, $\rho$, which represents the
newly observed agent's location at times $t \in [0, T]$?

\section{Our model}
We will assume that agents are of two types, linear or nonlinear.  Linear agents have dynamics specified by
\begin{align}
	\dot{x} (t)= x_0 + t \cdot v_0 \label{eq:lin}
\end{align}
where $v_0, x_0 \in \mathbb{R}^2$ are the position and velocity at time $t=0$.
Nonlinear agents come in flavors $\{1,\dots,n\}$ have dynamics given by
\begin{align}
	\dot{x} = s X_k(x) \label{eq:nonlin}
\end{align}
for some $k \in \{1,\dots,n\}$, a predetermined director-field, $X_k$, and some constant $s \in \mathbb{R}$.
This motion model allows us to decompose the computation of $\rho(t)$ into tractable components.
In particular $\rho(t,x)$ is defined as the probability density of finding the agent at position $x$ at time $t$ given measurements of position and velocity $\mu = (\hat{x}_0, \hat{v}_0)$ at time $t=0$.
In terms of the density at time $t=0$, $\rho$ is given by
\begin{align*}
	\rho(t,x ) := \Pr( x = x(t)\mid \mu )
\end{align*}
If we know the agent is linear and we know the velocity, then we know the position $x(t)$ by \eqref{eq:lin}.
If we know the agent is nonlinear, and we know the variables $k$ and $s$, then we know the position $x(t)$ by integrating \eqref{eq:nonlin}.
These observations suggest that we decompose the computation of $\rho$ using Baye's theorem, as
We find
\begin{align*}
	\rho(t,x ) &:= \int \Pr( x = x(t) \mid x(0) = x_0, \mu ) \cdot \Pr(x_0 \mid \mu ) dx_0 \\
	&= \sum_{k=1}^{n} \int \Pr( x = x(t) \mid k,s, x(0) = x_0, \mu ) \cdot P( k,s,x_0 \mid \mu) ds \, dx_0 \\
	&\quad + \int \Pr(x = x(t) \mid \Lin, x(0) = x_0, v_0 , \mu ) P( \Lin, x_0, v_0  \mid \mu ) dx_0 dv_0
\end{align*}

There are a lot of terms to deal with here.
Let us first deal with $\Pr( x = x(t) \mid k,s, x(0) = x_0, \mu )$.
Given \eqref{eq:nonlin} determines $x(t)$ precisely given and agent's flavor, $k$, speed, $s$, and initial condition,
we should expect a Dirac-delta type distribution which is independent of $\mu$.
Moreover, if $\Phi_k^t$ is the time-$t$ flow of $X_k$, then $\Phi_k^{st}$ is the time $t$ flow of $s X_k$. 
This means
\begin{align*}
	\Pr( x  = x(t) \mid k,s, x_0 = x_0 , \mu ) = \delta( x - \Phi_k^{st}( x_0) ).
\end{align*}

Similarly,
\begin{align*}
	\Pr( x = x(t) \mid \Lin, x(0) = x_0, v_0 , \mu ) = \delta \left( x - (x_0 + t \cdot v_0 ) \right)
\end{align*}

Substitution, and the change of variables formula tells us that
\begin{align*}
	\rho(t,x) &= \sum_{k=1}^n \int \det \left[ (D\Phi_{k}^{st})^{-1}(x)  \right] \cdot \Pr( k, s, x_0 = (\Phi_{k}^{st})^{-1} (x) \mid \mu) ds \\
		&\quad + \int \Pr( x_0 = x- t v_0 ,v_0, \Lin \mid \mu) \Pr( \Lin, x_0 =x- t v_0, v_0 \mid \mu )dv_0
\end{align*}

So if we can compute the flow $\Phi_k^{t}$ efficiently, and we can compute $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$ efficiently, then we are good to go.
Computing the flow can be done using standard ode integration schemes, so the main obstacle is computing the posteriors $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$.

\section{Expressions for $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$}
In order to compute $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$ we use a probabilistic graphical model.
To begin, define the set
\begin{align*}
	Type = \{ \Lin \} \cup \{ (k,s) \mid s \in [-\bar{s}, \bar{s}] , k = 1,\dots,n\}.
\end{align*}
Each agent is associated with one and only one type.
The type of a given agents provides information about his true position $x_0$ based on how often we've see an agent of such a type at location $x_0$.
Moreover, given an agents true position, and his type, we know something about his/her velocity.
For example, if an agent is of type $(k,s)$ and is known to be at position $x_0$, then we know with certainty that his velocity is $v_0 = sX_k(x_0)$.
Lastly, we assume our measurements of position are independent of Type, given the true position.
This yields the probabilistic graphical model
\begin{equation}
\begin{tikzcd}
	Type \arrow[r] \arrow[rd] & x_0 \arrow[d] \arrow[r] & \hat{x}_0 \\
	 & v_0 \ar[r] & \hat{v}_0 \\
\end{tikzcd}
\label{eq:pgm}
\end{equation}
Under this model, the posteriors $\Pr( k,s,x_0 \mid \mu)$ and $\Pr( x_0, v_0, \Lin \mid \mu)$ can be computed in terms of the \emph{atomic densities}
\begin{itemize}
	\item $\Pr( x_0 \mid Type )$
	\item $\Pr( v_0 \mid Type,x_0)$
	\item $\Pr( \hat{x}_0 \mid x_0)$
	\item $\Pr( \hat{v}_0 \mid v_0)$
	\item $\Pr(Type)$
\end{itemize}
In the remainder of this section, we will assume that these atomic densities are known, and easily computable, expressions.
We list precise choices for the atomic densities in the appendix.

To calculate $\Pr(k,s,x_0 \mid \mu )$ in terms of the atomic densities we use \eqref{eq:pgm} to find
\begin{align*}
	\Pr( k,s,x_0 \mid \mu ) &\propto \Pr( \mu \mid k,s,x_0) \Pr( k,s,x_0) \\
		&= \Pr( \hat{x}_0 \mid x_0) \left(  \int \Pr( \hat{v}_0 \mid v_0 ) \Pr( v_0 \mid k,s,x_0) dv_0 \right) \Pr(k,s,x_0) \\
		&=  \Pr( \hat{x}_0 \mid x_0) \left(  \int \Pr( \hat{v}_0 \mid v_0 ) \Pr( v_0 \mid k,s,x_0) dv_0 \right) \Pr(x_0 \mid k) \Pr(k) \Pr(s) \\
\end{align*}
Each of the posteriors in the last line is assumed known. \todo{Computing the normalizing constant might be expensive.  Can it be avoided?}

Similarly, in the linear case
\begin{align*}
	\Pr( \Lin , x_0, v_0 \mid \hat{x}_0, \hat{v}_0 ) &\propto \Pr( \hat{x}_0, \hat{v}_0 \mid \Lin, x_0, v_0 ) \Pr( \Lin, x_0, v_0 ) \\
	&= \Pr( \hat{x}_0, \hat{v}_0 \mid \Lin, x_0, v_0 ) \Pr( x_0, v_0 \mid \Lin ) \Pr(\Lin) \\
	&= \Pr( \hat{x}_0 \mid x_0 ) \Pr( \hat{v}_0 \mid v_0 ) \Pr( x_0 \mid \Lin ) \Pr( v_0 \mid x_0, \Lin ) \Pr(\Lin)
\end{align*}
Each of the above posteriors is assumed known.

\appendix

\section{Atomic Posteriors}
Let $w_x, w_v  > 0$ be a bound for the width of the bounding boxes and bounding box velocities we observe in the data.
Similarly, let $V_k: \mathbb{R}^2 \to \mathbb{R}$ be an energy function and $X_k \in \mathfrak{X}( \mathbb{R}^2)$ a director field we learn from the data. (see Appendix \ref{app:learning} )
Here are posteriors:
\begin{align*}
	\Pr( \hat{x}_0 \mid x_0 ) &\sim \mathcal{U}( x_0 ; w_x ) \\
	\Pr( \hat{v}_0 \mid v_0 ) &\sim \mathcal{U}( v_0 ; w_v ) \\
	\Pr( x_0 \mid k,s ) &= \Pr(x \mid k) = \frac{1}{Z_k} \exp \left( -V_k(x) \right) \\
	\Pr( v_0 \mid k,s,x_0) &= \delta( v_0 - s X_k(x_0) )
\end{align*}
The only remaining posterior to provide is $\Pr( x \mid \Lin )$ and $\Pr( v \mid x,\Lin )$.
We let $\Pr( x_0 \mid \Lin)$ be a uniform distribution over our domain.
The choice for $\Pr( v_0 \mid x_0,\Lin)$ is more subtle.
We want $\Pr(v \mid x, \Lin )$ to be small when there exists a $k$ such that $\Pr(x \mid k)$ is high and $v$ is aligned with $X_k(x)$.
So we consider 
\begin{align}
	\Pr( v_0 \mid x_0 , \Lin ) = \prod_{k=1}^n \left[ 1 - \left\| \frac{ v_0 \cdot X_{k}(x_0) }{ \epsilon + \| v_0 \|^2} \right\|^{\gamma_a} \tanh \left( \gamma_x e^{ - V_{k}(x_0) } \right)  \right] \label{eq:v given x and Linear}
\end{align}
where $\gamma_x, \gamma_a, \epsilon > 0$ are a tunable hyper-parameters.\todo{hoj:  Not so sure about \eqref{eq:v given x and Linear}.}
When $\gamma_x$ is large or when $\gamma_a$ is small we will tend to rely on the linear predictor more often.
$\gamma_a$ pertains to how much we care about alignment between the measured velocity, and the velocity of the given class, while $\gamma_x$ pertains to how close we like to be to the test set when using a one of the classes we've learned.
The hyper-parameter, $\epsilon$, is a regularization term.  The alignment between $X_{k}$ and $\eta_0$ is given by $\eta_0 \cdot X_{k}(\mu_0) / \| \eta_0 \|$.  This alignment measure is undefined when $\eta_0 = 0$ so the parameter $\epsilon > 0$ is included to regularize this singularity.  What this means in practice is that when the measured velocity is small, the predictor will default to a linear predictor.


\section{Objects learned from the data}
\label{app:learning}

\subsubsection{Learning clusters}
For a fixed scene with a database of agent trajectories we cluster the trajectories by applying the Affinity propagation algorithm to the end-points.
We then prune the clusters by discarding trajectories which are outliers with respect to total length (we define an outlier using the standard inter-quartile range criterion with a IQR coefficient of $1.5$).
We then throw out clusters which contain less than $10\%$ of the trajectories.
We associate class labels, $1,\dots,n$,  to the remaining clusters, and we will develop a model for each of these classes in the next section.
We also add an additional class, ``$\Lin$'', where the underlying model will be a linear predictor.
Finally, we define a prior, $P(c)$ to compute the probability that a given agent falls within one of these classes.
We set 
\begin{align*}
	P(\Lin) &= \frac{ \text{\# discarded trajectories} }{ \text{ \# trajectories } } \\
	P(k) &= \frac{ \text{\# trajectories in cluster $k$} }{ \text{ \# trajectories } } \text{ for } k=1,\dots,n.
\end{align*}

\subsubsection{Learning $P(x \mid k)$ }
During runtime we will need this computation to be fast, and to scale with the size of the data-set.
This rules out standard probabilistic classification schemes such kernel density methods.
Instead we discretize the space of energy functions, and compute on a subspace of fixed dimension.
For the linear-predictor class, $\Lin$, we assume $P(x \mid \Lin )$ is a uniform distribution over our domain.
For $k = 1, \dots, n$ we use the data to learn $P(x \mid k)$.
Given data points $x_{1,k},\dots,x_{N,k}$ associated to class $k$ we define
\begin{align}
	P(x \mid k) := \frac{1}{Z_k } e^{-V_k(x) } \label{eq:x given c}
\end{align}
where
$$
	V_k =  \text{argmin}_{V \in H_n}  \left( \log \left( \int e^{ - V(x) } dx \right) + \sum_{i=1}^N V(x_{i,k} ) \right).
$$
over some finite-dimensional subspace, $H_n \subset C^0( \mathbb{R}^2)$ (perhaps a space of low order polynomials).
This is a justifiable choice, since $V_k(x)$ is the most likely potential function in $H_n$, if the observations $\{ x_{i,k}\}_{k=1}^N$ are drawn from \eqref{eq:x given c}.
Again, the advantage to this approach is that we may restrict $V_k$ to a class of quickly computable functions which will not slow down performance at runtime.
In my current implementation, $H_n$ is a sum of tensor products of low order Legendre polynomials.

\subsubsection{Learning vector fields}
Given trajectories, we may fit a director field of the form $X(x,y) = ( \cos( \theta_\alpha(x,y) ) , \sin( \theta_\alpha(x,y) ) )$
where 
$$
	\theta_\alpha(x,y) = \sum_{ij} \alpha_{ij} L_i(x / w) L_j(y / h).
$$


Given observations $\vec{x}_0,  \dots, \vec{x}_n \in \mathbb{R}^2$ we may compute a series of unit vectors, $\vec{u}_k = \Delta \vec{x}_k / \| \Delta \vec{x}_k  \|$,
where $\Delta \vec{x}_k = \vec{x}_{k+1} - \vec{x}_k$.
A director-field may be learned from these directions by maximizing
$$
	R(\alpha ) = \sum_{k} \frac{x_{k+1} + x_k}{2} \cos( \theta_\alpha(\vec{x}_k) ) + \frac{y_{k+1} + y_k}{2}  \sin( \theta_\alpha( \vec{x}_k ) ).
$$
In words, this reward function measures the alignment of the director-field with an observation (i.e. the dot product), and takes sum over all observations.


\bibliographystyle{amsalpha}
\bibliography{hoj.bib}
\end{document}
