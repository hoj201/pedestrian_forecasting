\documentclass[12pt]{amsart}
\usepackage{amsmath,amssymb}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

%  POSSIBLY USEFULE PACKAGES
\usepackage{graphicx}
%\usepackage{tensor}
%\usepackage{todonotes}
\usepackage{verbatim}

%  NEW COMMANDS
\newcommand{\pder}[2]{\ensuremath{\frac{ \partial #1}{\partial #2}}}
\newcommand{\ppder}[3]{\ensuremath{\frac{\partial^2 #1}{\partial
      #2 \partial #3} } }

%  NEW THEOREM ENVIRONMENTS
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}


%  MATH OPERATORS
\DeclareMathOperator{\Diff}{Diff}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Ad}{Ad}

%  TITLE, AUTHOR, DATE
\title{Intent estimation}
\author{Henry O. Jacobs}
\date{June 2016}


\begin{document}

\maketitle

\begin{abstract}
    Some notes on predicting pedestrians.
\end{abstract}

\section{Summary of Karasev et. al.}

The algorithm of Karasev et al. is split into a few components.

\begin{enumerate}
	\item Convert goals into trajectories.
        \item Determine the probability of a goal given observables, $p(g_t \mid y^t)$.
        \item Compute an occupation map.
\end{enumerate}
The first item is addressed by posing the navigation problem as an MDP and using a standard reinforcement learning approach.
This implicitly gives one a map $g_t \to x_{t+k}, g_{t+k}$.
The second item, solving for $p(g_t \mid y^t)$ is done in Algorithm 1 using a particle filter known as a Rao-Blackwell filter.
This filter assumes we've alreader solved item (1), and know how to compute $x_{t+k}$ and $g_{t+k}$ as a function of $g_t$.
As we know how to convert goals into trajectories, the pdf of goals $p(g_t \mid y^t)$ can be converted into a pdf over future positions, $p( x_{t+k} \mid y^t)$.  This is done in the paper by sampling (see Alg 2). 
The occupation map is then computable from $p(x_{t+k} \mid y^t)$.

\section{Ram's critique}
There are multiple components here which we could improve upon.
Firstly, the computation of $p(x^{t+k} \mid y^t)$ might be more efficiently computed with the spectral techniques we've developed.
Secondly, the conversion from goals, $g$'s, to trajectories is done by posing the problem as a MDP, which means discretizing the space of goals to a finite set.
They extent their control law globally by linear interpolation later, but this seems hacky.
Discretizing space necessary discards problem structure.
In particular, that nearby goals of yield nearby trajectories.
We could consider something continuous in space to avoid discarding this structure.

\section{An idea}
We will take the same basic components as Karasev.
Namely, we will assume that the pedestrians are goal driven, and determine there trajectories as solutions of optimal control problems with unkown goals.
However, we can not pose an MDP as is done in Karasev, because computational techniques to obtain solutions require discretization (as far as I know).
We will pose a continous (in space and time) optimal control problem rather than a discrete problem.
In particular we will assume that pedestrian trajectories seek to minimize
\begin{align*}
    J[x(\cdot) ] = \int_0^1 L(x,\dot{x}) dt
\end{align*}
subject to the constraint $x(0) = x_0$ and $x(1) = g$, where $g$ is an unknown.
The optimal trajectories then satisfy a differential algebraic equation
\begin{align*}
    \frac{d}{dt} \left( \pder{L}{\dot{q}} \right) - \pder{L}{q} = 0.
\end{align*}
We can define the Euler-Lagrange operator $\mathcal{EL}: T^{(2)}Q \to T^*Q$ so that the Euler-Lagrange equations may be written as $\mathcal{EL}(q,\dot{q},\ddot{q}) = 0$.

Alternatively, if the Legendre-transform $\pder{L}{\dot{q}} : TQ \to T^*Q$ is invertible we may consider the Hamiltonian $H(q,p) = \langle p , \dot{q} \rangle - L(q,\dot{q})$ where $\dot{q}$ is a velocity such that $\pder{L}{\dot{q}}(q,\dot{q}) = (q,p)$.
The dynamics then take the form of an ordinary differential equation
\begin{align*}
	\dot{q} = \pder{H}{p} \quad,\quad \dot{p} = - \pder{H}{q}
\end{align*}
and Hamilton's principle entails extremizing
\begin{align*}
	J[ (q,p)(\cdot) ] = \int_0^1 \langle p , \dot{q} \rangle - H(q,p) dt
\end{align*}
with respect to the constraint $q(0) = q_0$ and $q(1) = q_1$.

%Our goal will be to learn $H$ (so that we get an ODE rather than a DAE).

\subsection{Learning $L$ from the Stanford pedestrian Data set}
Here is the most naive algorithm.  Let $e_k$ and $E_k$ denotes $L^2$ basis.
For any multi-index $\kappa = (k_1,k_2,k_3,k_4)$ let $e_\kappa = e_{k_1} \otimes E_{k_2} \otimes e_{k_3} \otimes E_{k_4}$.
We will assume $L$ takes the form
\begin{align*}
	L(x,\dot{x},y,\dot{y} ; \theta) = \sum_\kappa \theta_\kappa e_\kappa(x,\dot{x},y,\dot{y})
\end{align*}
The coefficients, $\theta_k$, are parameters which we will learn from the data.

In this case, the Euler-Lagrange operator takes the form
\begin{align*}
	&\mathcal{EL}_\theta (q,\dot{q},\ddot{q})^x = \sum_k \theta_k \big( \dot{x} e_{k_1}'(x) E_{k_2}'(\dot{x}) e_{k_3}(y) e_{k_4}(\dot{y}) \\
	&\quad + \ddot{x} e_{k_1}(x) E_{k_2}''(\dot{x}) e_{k_3}(y) E_{k_4}(\dot{y}) - e_{k_1}'(x) E_{k_2}(\dot{x}) e_{k_3}(y) E_{k_4}(\dot{y}) \big)  \\
\end{align*}
and a similar form applies to $\mathcal{EL}_\theta^y$.
Note that this expression is linear in $\theta$.
If we use the identities
\begin{align*}
	H_n'(\dot{x}) &= 2n H_{n-1}(\dot{x}) \\
	H_n'(\dot{x}) \dot{x} &= n H_n(\dot{x}) - 2n^2 H_{n-1}(\dot{x})
\end{align*}

Given estimates of $q_i, \dot{q}_i$ and $\ddot{q}_i$ for $i=1,\dots, N$ we can choose the coefficients to satisfy the left hand side.
However, $\mathcal{EL}$ has a kernel with respect to $\theta$ becuase
it does not distinguish between $L$ and $\alpha L + \beta$ for $\alpha,\beta \in \mathbb{R}$.
To handle this, we can constrain $c_0 = 0$, and include a linear constraint like $\sum_k c_k = 1$.
If we use the quadratic cost function
\begin{align*}
	C( \theta ) = \sum_{i=1}^{N} \| \mathcal{EL}_\theta( q_i ,\dot{q}_i ,\ddot{q}_i  ) \|^2
\end{align*}
we can pose our problem as a quadratic program of the form ``$\sup x^T Q x$ subject to $Ex = d$''.
The solution to this form of QP is found by using Lagrange multipliers and solving the linear system

\begin{align*}
	\begin{bmatrix}
		Q & E^T \\
		E & 0 
	\end{bmatrix}
	\begin{bmatrix}
		x \\
		\lambda
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		d
	\end{bmatrix}
\end{align*}
In our case $d=1$, $E = [1,\dots,1]$ and $Q$ is the quadratic form associated to $C$ with $\theta = x$.

\subsection{Coding the Euler-Lagrange operator as a linear functional}
For a given $a=(q,\dot{q},\ddot{q})$ we observe that $\mathcal{EL}_\theta(a)^x$ is a linear functional of $\theta$, which we can call $\ell^x(a)$.
We could then express the desired quadratic form as
\begin{align*}
Q^x = \sum_{a \in \mathcal{O}} \ell^x(a) \otimes \ell^x(a)^T
\end{align*}
where $\mathcal{O}$ is some set of observations.

Numpy and MATLAB both have efficient kronecker product routines which we should take advantage of.
Explicitly, $\ell^x(a) = (M^x(a) \otimes I) \cdot e(a)$ with
\begin{align*}
	M^x(a) = D_x \otimes ( \dot{x} D_{\dot{x}}) + I \otimes \ddot{x} D_{\dot{x}}^2 - D_x \otimes I \\
	e_k(a) = e_{k_1}(x) \otimes e_{k_2}(\dot{x}) \otimes e_{k_3}(y) \otimes e_{k_4}(\dot{y}) 
\end{align*}

Similarly, we may consider the quadratic form $Q^y$ defined such that $\| \mathcal{EL}_\theta(a)^y \|^2 = \theta^T \cdot Q^y(a) \cdot \theta$.
In this case $Q^y = \sum_{a \in \mathcal{O}} \ell^y(a) \otimes \ell^y(a)^T$ with $\ell^y(a) = (I \otimes M^y(a) ) \cdot e(a)$ with $M^y(a)$ defined in a way similar to $M^x(a)$.
Our desired quadratic form is then $Q = Q^x + Q^y$.

\section{Spectral Discretization}
\label{sec:spectral}
Let us assume $V$ is of the form
\begin{align*}
	V(x,y) = \sum_k c_k \exp \left( 2\pi i ( k_1 x + k_2 y) \right)
\end{align*}
The dynamics are given by
\begin{align*}
	\dot{x} &= u \\
	\dot{y} &= v \\
	\dot{u} &= - \partial_x V(x,y) = -2\pi i \sum_k k_1 c_k \exp \left( 2\pi i ( k_1 x + k_2 y) \right)  \\
	\dot{v} &= - \partial_y V(x,y) = -2\pi i \sum_k k_2 c_k \exp \left( 2\pi i ( k_1 x + k_2 y) \right)
\end{align*}
We desire to integrate the PDE
\begin{align*}
	\partial_t \psi + \frac{1}{2} \left(  X^i \cdot \partial_i \psi + \partial_i (X_i \cdot \psi) \right) = 0
\end{align*}
on the space $\mathbb{T}^2 \times \mathbb{R}^2$.
We will do so spectrally using basis functions of the form
\begin{align*}
	e_{k}(x,y,u,v) = e^{ 2\pi i( k_1 x+k_2 y) } h_{k_3}(u) h_{k_4}(v)
\end{align*}

\section{Scales}
Figure \ref{fig:coupa} depicts the reference image from {\verb Coupa/video_0/}.  The distance between the planters is about 12ft.  This was gleaned from the Google maps scale bar.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=3in]{coupa_0.jpg} 
   \caption{Reference image for Coupa, video 0. The distance between the planters is about 12ft.}
   \label{fig:coupa}
\end{figure}

\appendix
\section{Reference frame mapping}
We want to map $(x_1,y_1),(x_2,y_2),(x_3,y_3) \in \mathbb{R}^2$ to the points $(X_1,Y_1),(X_2,Y_2),(X_3,Y_3)$ using an affine transformation.
That is, we want to find scalars $a,b,c,d,e,$ and $f$ such that
\begin{align*}
	\begin{bmatrix}
		a & b \\
		c & d
	\end{bmatrix}
	\begin{bmatrix}
		x_i \\
		y_i
	\end{bmatrix}
	+ 
	\begin{bmatrix}
	e\\
	f
	\end{bmatrix} = 
	\begin{bmatrix}
	X_i \\
	Y_i
	\end{bmatrix}
\end{align*} 
for $i=1,2,3$.

This comes down to solving the linear problem
\begin{align*}
	\begin{bmatrix}
	x_1 & y_1 & 0 & 0 & 1 & 0 \\
	x_2 & y_2 & 0 & 0 & 1 & 0 \\
	x_3 & y_3 & 0 & 0 & 1 & 0 \\
	0 & 0 & x_1 & y_1 & 0 & 1 \\
	0 & 0 & x_2 & y_2 & 0 & 1 \\
	0 & 0 & x_3 & y_3 & 0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		a \\ b \\ c \\ d \\ e \\ f 
	\end{bmatrix}
	=
	\begin{bmatrix}
		X_1 \\
		Y_1 \\
		X_2 \\
		Y_2 \\
		X_3 \\
		Y_3
	\end{bmatrix}
\end{align*}

\end{document}
